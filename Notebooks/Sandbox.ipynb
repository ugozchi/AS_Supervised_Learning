{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Début de la lecture 'bypass' ---\n",
      "Lecture du fichier via PyArrow : ../Data/raw/StockEtablissement_utf8.parquet\n",
      "Conversion de la table PyArrow en DataFrame Polars...\n",
      "--- SUCCÈS ! ---\n",
      "\n",
      "Le DataFrame est maintenant dans Polars, prêt pour la transformation.\n",
      "shape: (5, 18)\n",
      "┌───────────┬─────┬────────────┬────────────┬───┬────────────┬────────────┬────────────┬───────────┐\n",
      "│ siren     ┆ nic ┆ siret      ┆ dateFin    ┆ … ┆ nomenclatu ┆ changement ┆ caractereE ┆ changemen │\n",
      "│ ---       ┆ --- ┆ ---        ┆ ---        ┆   ┆ reActivite ┆ ActivitePr ┆ mployeurEt ┆ tCaracter │\n",
      "│ str       ┆ i64 ┆ str        ┆ date       ┆   ┆ Principale ┆ incipaleEt ┆ ablissemen ┆ eEmployeu │\n",
      "│           ┆     ┆            ┆            ┆   ┆ …          ┆ …          ┆ …          ┆ rEt…      │\n",
      "│           ┆     ┆            ┆            ┆   ┆ ---        ┆ ---        ┆ ---        ┆ ---       │\n",
      "│           ┆     ┆            ┆            ┆   ┆ str        ┆ bool       ┆ str        ┆ bool      │\n",
      "╞═══════════╪═════╪════════════╪════════════╪═══╪════════════╪════════════╪════════════╪═══════════╡\n",
      "│ 000325175 ┆ 16  ┆ 0003251750 ┆ null       ┆ … ┆ NAFRev2    ┆ false      ┆ N          ┆ false     │\n",
      "│           ┆     ┆ 0016       ┆            ┆   ┆            ┆            ┆            ┆           │\n",
      "│ 000325175 ┆ 16  ┆ 0003251750 ┆ 2009-05-26 ┆ … ┆ NAFRev2    ┆ true       ┆ N          ┆ false     │\n",
      "│           ┆     ┆ 0016       ┆            ┆   ┆            ┆            ┆            ┆           │\n",
      "│ 000325175 ┆ 16  ┆ 0003251750 ┆ 2007-12-31 ┆ … ┆ null       ┆ false      ┆ N          ┆ false     │\n",
      "│           ┆     ┆ 0016       ┆            ┆   ┆            ┆            ┆            ┆           │\n",
      "│ 000325175 ┆ 24  ┆ 0003251750 ┆ null       ┆ … ┆ NAFRev2    ┆ false      ┆ N          ┆ false     │\n",
      "│           ┆     ┆ 0024       ┆            ┆   ┆            ┆            ┆            ┆           │\n",
      "│ 000325175 ┆ 24  ┆ 0003251750 ┆ 2011-10-20 ┆ … ┆ NAFRev2    ┆ false      ┆ N          ┆ false     │\n",
      "│           ┆     ┆ 0024       ┆            ┆   ┆            ┆            ┆            ┆           │\n",
      "└───────────┴─────┴────────────┴────────────┴───┴────────────┴────────────┴────────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pyarrow.parquet as pq\n",
    "import sys\n",
    "\n",
    "filepath = \"../Data/raw/StockEtablissement_utf8.parquet\" \n",
    "\n",
    "print(\"--- Début de la lecture 'bypass' ---\")\n",
    "\n",
    "try:\n",
    "    print(f\"Lecture du fichier via PyArrow : {filepath}\")\n",
    "    table_arrow = pq.read_table(\n",
    "        filepath,\n",
    "    )\n",
    "    \n",
    "    print(\"Conversion de la table PyArrow en DataFrame Polars...\")\n",
    "    df_eta = pl.from_arrow(table_arrow)\n",
    "    \n",
    "    print(\"--- SUCCÈS ! ---\\n\")\n",
    "    print(\"Le DataFrame est maintenant dans Polars, prêt pour la transformation.\")\n",
    "    print(df_eta.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- ERREUR ---\", file=sys.stderr)\n",
    "    print(f\"Impossible de lire le fichier, même avec PyArrow : {e}\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 18)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>siren</th><th>nic</th><th>siret</th><th>dateFin</th><th>dateDebut</th><th>etatAdministratifEtablissement</th><th>changementEtatAdministratifEtablissement</th><th>enseigne1Etablissement</th><th>enseigne2Etablissement</th><th>enseigne3Etablissement</th><th>changementEnseigneEtablissement</th><th>denominationUsuelleEtablissement</th><th>changementDenominationUsuelleEtablissement</th><th>activitePrincipaleEtablissement</th><th>nomenclatureActivitePrincipaleEtablissement</th><th>changementActivitePrincipaleEtablissement</th><th>caractereEmployeurEtablissement</th><th>changementCaractereEmployeurEtablissement</th></tr><tr><td>str</td><td>i64</td><td>str</td><td>date</td><td>date</td><td>str</td><td>bool</td><td>str</td><td>str</td><td>str</td><td>bool</td><td>str</td><td>bool</td><td>str</td><td>str</td><td>bool</td><td>str</td><td>bool</td></tr></thead><tbody><tr><td>&quot;000325175&quot;</td><td>16</td><td>&quot;00032517500016&quot;</td><td>null</td><td>2009-05-27</td><td>&quot;F&quot;</td><td>true</td><td>null</td><td>null</td><td>null</td><td>false</td><td>null</td><td>false</td><td>&quot;32.12Z&quot;</td><td>&quot;NAFRev2&quot;</td><td>false</td><td>&quot;N&quot;</td><td>false</td></tr><tr><td>&quot;000325175&quot;</td><td>16</td><td>&quot;00032517500016&quot;</td><td>2009-05-26</td><td>2008-01-01</td><td>&quot;A&quot;</td><td>false</td><td>null</td><td>null</td><td>null</td><td>false</td><td>null</td><td>false</td><td>&quot;32.12Z&quot;</td><td>&quot;NAFRev2&quot;</td><td>true</td><td>&quot;N&quot;</td><td>false</td></tr><tr><td>&quot;000325175&quot;</td><td>16</td><td>&quot;00032517500016&quot;</td><td>2007-12-31</td><td>2000-09-26</td><td>&quot;A&quot;</td><td>false</td><td>null</td><td>null</td><td>null</td><td>false</td><td>null</td><td>false</td><td>null</td><td>null</td><td>false</td><td>&quot;N&quot;</td><td>false</td></tr><tr><td>&quot;000325175&quot;</td><td>24</td><td>&quot;00032517500024&quot;</td><td>null</td><td>2011-10-21</td><td>&quot;F&quot;</td><td>true</td><td>&quot;TAHITI PERLES CREATIONS&quot;</td><td>null</td><td>null</td><td>false</td><td>null</td><td>false</td><td>&quot;47.89Z&quot;</td><td>&quot;NAFRev2&quot;</td><td>false</td><td>&quot;N&quot;</td><td>false</td></tr><tr><td>&quot;000325175&quot;</td><td>24</td><td>&quot;00032517500024&quot;</td><td>2011-10-20</td><td>2008-05-20</td><td>&quot;A&quot;</td><td>false</td><td>&quot;TAHITI PERLES CREATIONS&quot;</td><td>null</td><td>null</td><td>false</td><td>null</td><td>false</td><td>&quot;47.89Z&quot;</td><td>&quot;NAFRev2&quot;</td><td>false</td><td>&quot;N&quot;</td><td>false</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 18)\n",
       "┌───────────┬─────┬────────────┬────────────┬───┬────────────┬────────────┬────────────┬───────────┐\n",
       "│ siren     ┆ nic ┆ siret      ┆ dateFin    ┆ … ┆ nomenclatu ┆ changement ┆ caractereE ┆ changemen │\n",
       "│ ---       ┆ --- ┆ ---        ┆ ---        ┆   ┆ reActivite ┆ ActivitePr ┆ mployeurEt ┆ tCaracter │\n",
       "│ str       ┆ i64 ┆ str        ┆ date       ┆   ┆ Principale ┆ incipaleEt ┆ ablissemen ┆ eEmployeu │\n",
       "│           ┆     ┆            ┆            ┆   ┆ …          ┆ …          ┆ …          ┆ rEt…      │\n",
       "│           ┆     ┆            ┆            ┆   ┆ ---        ┆ ---        ┆ ---        ┆ ---       │\n",
       "│           ┆     ┆            ┆            ┆   ┆ str        ┆ bool       ┆ str        ┆ bool      │\n",
       "╞═══════════╪═════╪════════════╪════════════╪═══╪════════════╪════════════╪════════════╪═══════════╡\n",
       "│ 000325175 ┆ 16  ┆ 0003251750 ┆ null       ┆ … ┆ NAFRev2    ┆ false      ┆ N          ┆ false     │\n",
       "│           ┆     ┆ 0016       ┆            ┆   ┆            ┆            ┆            ┆           │\n",
       "│ 000325175 ┆ 16  ┆ 0003251750 ┆ 2009-05-26 ┆ … ┆ NAFRev2    ┆ true       ┆ N          ┆ false     │\n",
       "│           ┆     ┆ 0016       ┆            ┆   ┆            ┆            ┆            ┆           │\n",
       "│ 000325175 ┆ 16  ┆ 0003251750 ┆ 2007-12-31 ┆ … ┆ null       ┆ false      ┆ N          ┆ false     │\n",
       "│           ┆     ┆ 0016       ┆            ┆   ┆            ┆            ┆            ┆           │\n",
       "│ 000325175 ┆ 24  ┆ 0003251750 ┆ null       ┆ … ┆ NAFRev2    ┆ false      ┆ N          ┆ false     │\n",
       "│           ┆     ┆ 0024       ┆            ┆   ┆            ┆            ┆            ┆           │\n",
       "│ 000325175 ┆ 24  ┆ 0003251750 ┆ 2011-10-20 ┆ … ┆ NAFRev2    ┆ false      ┆ N          ┆ false     │\n",
       "│           ┆     ┆ 0024       ┆            ┆   ┆            ┆            ┆            ┆           │\n",
       "└───────────┴─────┴────────────┴────────────┴───┴────────────┴────────────┴────────────┴───────────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Début de la lecture 'bypass' ---\n",
      "Lecture du fichier via PyArrow : ../Data/processed/sirene_bilan.parquet\n",
      "Conversion de la table PyArrow en DataFrame Polars...\n",
      "--- SUCCÈS ! ---\n",
      "\n",
      "Le DataFrame est maintenant dans Polars, prêt pour la transformation.\n",
      "shape: (5, 21)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ siren     ┆ date_clot ┆ CJCK_Tota ┆ EG_Impots ┆ … ┆ ratio_cap ┆ ratio_tre ┆ ratio_res ┆ ratio_re │\n",
      "│ ---       ┆ ure_exerc ┆ lActifBru ┆ Taxes     ┆   ┆ itaux_pro ┆ sorerie   ┆ ultat_fin ┆ sultat_e │\n",
      "│ str       ┆ ice       ┆ t         ┆ ---       ┆   ┆ pres      ┆ ---       ┆ ancier    ┆ xception │\n",
      "│           ┆ ---       ┆ ---       ┆ i32       ┆   ┆ ---       ┆ f64       ┆ ---       ┆ nel      │\n",
      "│           ┆ date      ┆ i32       ┆           ┆   ┆ f64       ┆           ┆ f64       ┆ ---      │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ f64      │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│ 005420120 ┆ 2018-12-3 ┆ 15117606  ┆ 841098    ┆ … ┆ 0.0       ┆ 0.047087  ┆ 29.374216 ┆ 101.8593 │\n",
      "│           ┆ 1         ┆           ┆           ┆   ┆           ┆           ┆           ┆ 99       │\n",
      "│ 005420120 ┆ 2021-12-3 ┆ 10813111  ┆ 2500000   ┆ … ┆ 0.0       ┆ 0.065831  ┆ 6.162617  ┆ 14.06566 │\n",
      "│           ┆ 1         ┆           ┆           ┆   ┆           ┆           ┆           ┆ 4        │\n",
      "│ 005420120 ┆ 2017-12-3 ┆ 22684824  ┆ 441247    ┆ … ┆ 0.0       ┆ 0.03138   ┆ 3.745877  ┆ 17.20460 │\n",
      "│           ┆ 1         ┆           ┆           ┆   ┆           ┆           ┆           ┆ 4        │\n",
      "│ 005420120 ┆ 2016-12-3 ┆ 31933093  ┆ 586967    ┆ … ┆ 0.0       ┆ 0.022292  ┆ 8.805762  ┆ 66.05635 │\n",
      "│           ┆ 1         ┆           ┆           ┆   ┆           ┆           ┆           ┆ 3        │\n",
      "│ 005420120 ┆ 2019-12-3 ┆ 12736527  ┆ 0         ┆ … ┆ 0.0       ┆ 0.05589   ┆ 4.502626  ┆ 10.75797 │\n",
      "│           ┆ 1         ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pyarrow.parquet as pq\n",
    "import sys\n",
    "\n",
    "filepath = \"../Data/processed/sirene_bilan.parquet\" \n",
    "\n",
    "print(\"--- Début de la lecture 'bypass' ---\")\n",
    "\n",
    "try:\n",
    "    print(f\"Lecture du fichier via PyArrow : {filepath}\")\n",
    "    table_arrow = pq.read_table(\n",
    "        filepath,\n",
    "    )\n",
    "    \n",
    "    print(\"Conversion de la table PyArrow en DataFrame Polars...\")\n",
    "    df_bilan = pl.from_arrow(table_arrow)\n",
    "    \n",
    "    print(\"--- SUCCÈS ! ---\\n\")\n",
    "    print(\"Le DataFrame est maintenant dans Polars, prêt pour la transformation.\")\n",
    "    print(df_bilan.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- ERREUR ---\", file=sys.stderr)\n",
    "    print(f\"Impossible de lire le fichier, même avec PyArrow : {e}\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Schema([('siren', String),\n",
       "        ('date_cloture_exercice', Date),\n",
       "        ('CJCK_TotalActifBrut', Int32),\n",
       "        ('EG_ImpotsTaxes', Int32),\n",
       "        ('FJ_ResultatFinancier', Int32),\n",
       "        ('FA_ChiffreAffairesVentes', Int32),\n",
       "        ('HN_RésultatNet', Int32),\n",
       "        ('DA_TresorerieActive', Int32),\n",
       "        ('DL_DettesCourtTerme', Int32),\n",
       "        ('FB_AchatsMarchandises', Int32),\n",
       "        ('FR_ResultatExceptionnel', Int32),\n",
       "        ('DF_CapitauxPropres', Int32),\n",
       "        ('DM_DettesLongTerme', Int32),\n",
       "        ('AnneeClotureExercice', Int32),\n",
       "        ('ratio_rentabilite_nette', Float64),\n",
       "        ('ratio_endettement', Float64),\n",
       "        ('ratio_marge_brute', Float64),\n",
       "        ('ratio_capitaux_propres', Float64),\n",
       "        ('ratio_tresorerie', Float64),\n",
       "        ('ratio_resultat_financier', Float64),\n",
       "        ('ratio_resultat_exceptionnel', Float64)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bilan.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 22)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>siren</th><th>date_cloture_exercice</th><th>CJCK_TotalActifBrut</th><th>EG_ImpotsTaxes</th><th>FJ_ResultatFinancier</th><th>FA_ChiffreAffairesVentes</th><th>HN_RésultatNet</th><th>DA_TresorerieActive</th><th>DL_DettesCourtTerme</th><th>FB_AchatsMarchandises</th><th>FR_ResultatExceptionnel</th><th>DF_CapitauxPropres</th><th>DM_DettesLongTerme</th><th>AnneeClotureExercice</th><th>ratio_rentabilite_nette</th><th>ratio_endettement</th><th>ratio_marge_brute</th><th>ratio_capitaux_propres</th><th>ratio_tresorerie</th><th>ratio_resultat_financier</th><th>ratio_resultat_exceptionnel</th></tr><tr><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>&quot;3706645&quot;</td><td>&quot;3706645&quot;</td><td>3.706645e6</td><td>3.706645e6</td><td>3.706645e6</td><td>3.706645e6</td><td>3.706645e6</td><td>3.706645e6</td><td>3.706645e6</td><td>3.706645e6</td><td>3.706645e6</td><td>3.706645e6</td><td>3.706645e6</td><td>3.706645e6</td><td>3.706645e6</td><td>3.706645e6</td><td>3.706645e6</td><td>3.706645e6</td><td>3.706645e6</td><td>3.706645e6</td><td>3.706645e6</td></tr><tr><td>&quot;null_count&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>null</td><td>&quot;2019-10-06 17:39:49.194433&quot;</td><td>3.9138e6</td><td>1.1025e6</td><td>3.3713e6</td><td>1.4836e6</td><td>361013.787403</td><td>1.9523e6</td><td>4.2339e6</td><td>192032.807618</td><td>5.6904e6</td><td>30394.966507</td><td>29153.569909</td><td>2018.926753</td><td>2.9659e11</td><td>4.1662e11</td><td>-1.7666e10</td><td>8.7383e8</td><td>2.1672e11</td><td>1.1745e12</td><td>3.0131e12</td></tr><tr><td>&quot;std&quot;</td><td>null</td><td>null</td><td>4.3182e7</td><td>1.6389e7</td><td>3.6384e7</td><td>2.7870e7</td><td>1.7763e7</td><td>3.7255e7</td><td>6.0270e7</td><td>9.3891e6</td><td>6.4034e7</td><td>4.4865e6</td><td>5.8759e6</td><td>2.143208</td><td>1.7165e13</td><td>2.6457e13</td><td>3.1909e12</td><td>1.1443e12</td><td>1.7773e13</td><td>1.6774e13</td><td>5.1192e13</td></tr><tr><td>&quot;min&quot;</td><td>&quot;005420120&quot;</td><td>&quot;1919-09-30&quot;</td><td>-1.0535e9</td><td>-2.1475e9</td><td>-2.1460e9</td><td>-5.4749e8</td><td>-2.1475e9</td><td>-2.1475e9</td><td>-2.1475e9</td><td>-5.25e7</td><td>-2.1475e9</td><td>-1.3826e9</td><td>-360972.0</td><td>1919.0</td><td>-2.1475e15</td><td>-2.1475e15</td><td>-1.8847e15</td><td>-7.2972e12</td><td>-2.1475e15</td><td>-2.1460e15</td><td>-2.1475e15</td></tr><tr><td>&quot;25%&quot;</td><td>null</td><td>&quot;2017-12-31&quot;</td><td>111338.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>7622.0</td><td>48456.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>2017.0</td><td>0.0</td><td>0.257183</td><td>0.0</td><td>0.0</td><td>0.024045</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;50%&quot;</td><td>null</td><td>&quot;2019-12-31&quot;</td><td>425115.0</td><td>82447.0</td><td>42406.0</td><td>0.0</td><td>2619.0</td><td>30000.0</td><td>260868.0</td><td>0.0</td><td>164478.0</td><td>0.0</td><td>0.0</td><td>2019.0</td><td>0.026677</td><td>0.609753</td><td>0.0</td><td>0.0</td><td>0.084052</td><td>1.003174</td><td>60.752917</td></tr><tr><td>&quot;75%&quot;</td><td>null</td><td>&quot;2021-09-30&quot;</td><td>1.22434e6</td><td>431133.0</td><td>878152.0</td><td>0.0</td><td>66630.0</td><td>155000.0</td><td>832928.0</td><td>0.0</td><td>1.291498e6</td><td>0.0</td><td>0.0</td><td>2021.0</td><td>2.6091e10</td><td>1.045684</td><td>0.0</td><td>0.0</td><td>0.332404</td><td>1.3592e11</td><td>3.7060e11</td></tr><tr><td>&quot;max&quot;</td><td>&quot;999990542&quot;</td><td>&quot;2029-12-31&quot;</td><td>2.1475e9</td><td>2.1475e9</td><td>2.1475e9</td><td>2.1475e9</td><td>2.1475e9</td><td>2.1475e9</td><td>2.1475e9</td><td>2.1475e9</td><td>2.1475e9</td><td>2.1475e9</td><td>2.1475e9</td><td>2029.0</td><td>2.1475e15</td><td>2.1475e15</td><td>1.4149e12</td><td>2.1475e15</td><td>2.1475e15</td><td>2.1475e15</td><td>2.1475e15</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 22)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ statistic ┆ siren     ┆ date_clot ┆ CJCK_Tota ┆ … ┆ ratio_cap ┆ ratio_tre ┆ ratio_res ┆ ratio_re │\n",
       "│ ---       ┆ ---       ┆ ure_exerc ┆ lActifBru ┆   ┆ itaux_pro ┆ sorerie   ┆ ultat_fin ┆ sultat_e │\n",
       "│ str       ┆ str       ┆ ice       ┆ t         ┆   ┆ pres      ┆ ---       ┆ ancier    ┆ xception │\n",
       "│           ┆           ┆ ---       ┆ ---       ┆   ┆ ---       ┆ f64       ┆ ---       ┆ nel      │\n",
       "│           ┆           ┆ str       ┆ f64       ┆   ┆ f64       ┆           ┆ f64       ┆ ---      │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ f64      │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ count     ┆ 3706645   ┆ 3706645   ┆ 3.706645e ┆ … ┆ 3.706645e ┆ 3.706645e ┆ 3.706645e ┆ 3.706645 │\n",
       "│           ┆           ┆           ┆ 6         ┆   ┆ 6         ┆ 6         ┆ 6         ┆ e6       │\n",
       "│ null_coun ┆ 0         ┆ 0         ┆ 0.0       ┆ … ┆ 0.0       ┆ 0.0       ┆ 0.0       ┆ 0.0      │\n",
       "│ t         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ mean      ┆ null      ┆ 2019-10-0 ┆ 3.9138e6  ┆ … ┆ 8.7383e8  ┆ 2.1672e11 ┆ 1.1745e12 ┆ 3.0131e1 │\n",
       "│           ┆           ┆ 6 17:39:4 ┆           ┆   ┆           ┆           ┆           ┆ 2        │\n",
       "│           ┆           ┆ 9.194433  ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ std       ┆ null      ┆ null      ┆ 4.3182e7  ┆ … ┆ 1.1443e12 ┆ 1.7773e13 ┆ 1.6774e13 ┆ 5.1192e1 │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 3        │\n",
       "│ min       ┆ 005420120 ┆ 1919-09-3 ┆ -1.0535e9 ┆ … ┆ -7.2972e1 ┆ -2.1475e1 ┆ -2.1460e1 ┆ -2.1475e │\n",
       "│           ┆           ┆ 0         ┆           ┆   ┆ 2         ┆ 5         ┆ 5         ┆ 15       │\n",
       "│ 25%       ┆ null      ┆ 2017-12-3 ┆ 111338.0  ┆ … ┆ 0.0       ┆ 0.024045  ┆ 0.0       ┆ 0.0      │\n",
       "│           ┆           ┆ 1         ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 50%       ┆ null      ┆ 2019-12-3 ┆ 425115.0  ┆ … ┆ 0.0       ┆ 0.084052  ┆ 1.003174  ┆ 60.75291 │\n",
       "│           ┆           ┆ 1         ┆           ┆   ┆           ┆           ┆           ┆ 7        │\n",
       "│ 75%       ┆ null      ┆ 2021-09-3 ┆ 1.22434e6 ┆ … ┆ 0.0       ┆ 0.332404  ┆ 1.3592e11 ┆ 3.7060e1 │\n",
       "│           ┆           ┆ 0         ┆           ┆   ┆           ┆           ┆           ┆ 1        │\n",
       "│ max       ┆ 999990542 ┆ 2029-12-3 ┆ 2.1475e9  ┆ … ┆ 2.1475e15 ┆ 2.1475e15 ┆ 2.1475e15 ┆ 2.1475e1 │\n",
       "│           ┆           ┆ 1         ┆           ┆   ┆           ┆           ┆           ┆ 5        │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bilan.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Lancement Data Prep (Version Notebook) ---\n",
      "--- Lancement Script 01: Création du MASTER FILE 'Démo Monstrueuse' ---\n",
      "Fichiers bruts trouvés. Lancement de la pipeline...\n",
      "Étape 1: Lecture des features de 'StockUniteLegale'...\n",
      "Étape 2: Lecture de 'StockEtablissement' pour trouver les sièges...\n",
      "Étape 3: Lecture de 'StockEtablissementHistorique' pour trouver les 'morts'...\n",
      "Étape 4: Jointure finale des 3 tables...\n",
      "Sauvegarde du Master File 'Démo Monstrueuse' dans ../Data/processed/sirene_infos_MONSTROUS_DEMO.parquet...\n",
      "--- Script 01 (Démo Monstrueuse) Terminé avec Succès ---\n",
      "Fichier créé : ../Data/processed/sirene_infos_MONSTROUS_DEMO.parquet\n",
      "Shape finale : (28882409, 13)\n",
      "shape: (5, 13)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ siren     ┆ dateCreat ┆ dateFerme ┆ categorie ┆ … ┆ moisCreat ┆ departeme ┆ trancheEf ┆ caracter │\n",
      "│ ---       ┆ ionUniteL ┆ ture      ┆ Juridique ┆   ┆ ion       ┆ nt        ┆ fectifsSi ┆ eEmploye │\n",
      "│ str       ┆ egale     ┆ ---       ┆ UniteLega ┆   ┆ ---       ┆ ---       ┆ ege       ┆ urSiege  │\n",
      "│           ┆ ---       ┆ date      ┆ le        ┆   ┆ i8        ┆ str       ┆ ---       ┆ ---      │\n",
      "│           ┆ date      ┆           ┆ ---       ┆   ┆           ┆           ┆ str       ┆ str      │\n",
      "│           ┆           ┆           ┆ i64       ┆   ┆           ┆           ┆           ┆          │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│ 000325175 ┆ 2000-09-2 ┆ null      ┆ 1000      ┆ … ┆ 9         ┆ 13        ┆ NN        ┆ N        │\n",
      "│           ┆ 6         ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 001807254 ┆ 1972-05-0 ┆ null      ┆ 1000      ┆ … ┆ 5         ┆ 02        ┆ NN        ┆ N        │\n",
      "│           ┆ 1         ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 005410220 ┆ 1954-12-2 ┆ null      ┆ 1000      ┆ … ┆ 12        ┆ 80        ┆ NN        ┆ N        │\n",
      "│           ┆ 5         ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 005410345 ┆ null      ┆ null      ┆ 1000      ┆ … ┆ null      ┆ 80        ┆ NN        ┆ N        │\n",
      "│ 005410394 ┆ 1954-12-2 ┆ null      ┆ 1000      ┆ … ┆ 12        ┆ 80        ┆ NN        ┆ N        │\n",
      "│           ┆ 5         ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"--- Lancement Data Prep (Version Notebook) ---\")\n",
    "\n",
    "# --- 1. DÉFINIR LES CHEMINS (Version Notebook) ---\n",
    "# On part de /Notebooks/ et on remonte avec \"../\"\n",
    "PATH_UL = \"../Data/raw/StockUniteLegale_utf8.parquet\"\n",
    "PATH_ETAB = \"../Data/raw/StockEtablissement_utf8.parquet\"\n",
    "PATH_ETAB_HISTO = \"../Data/raw/StockEtablissementHistorique_utf8.parquet\"\n",
    "PATH_OUTPUT = \"../Data/processed/sirene_infos_MONSTROUS_DEMO.parquet\" # C'est notre nouveau \"Master File\"\n",
    "\n",
    "print(\"--- Lancement Script 01: Création du MASTER FILE 'Démo Monstrueuse' ---\")\n",
    "\n",
    "# --- 2. VÉRIFICATION DES FICHIERS ---\n",
    "for path in [PATH_UL, PATH_ETAB, PATH_ETAB_HISTO]:\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"ERREUR FATALE: Fichier brut manquant : {path}\", file=sys.stderr)\n",
    "        print(\"Vérifie que tes 3 fichiers SIRENE sont bien dans '../Data/raw/'\")\n",
    "        raise FileNotFoundError(f\"Fichier non trouvé : {path}\")\n",
    "print(\"Fichiers bruts trouvés. Lancement de la pipeline...\")\n",
    "\n",
    "# ===================================================================\n",
    "# ÉTAPE 1: La Base (FEATURES X) - Fichier 'StockUniteLegale'\n",
    "# (On prend 8 features au lieu de 5)\n",
    "# ===================================================================\n",
    "print(\"Étape 1: Lecture des features de 'StockUniteLegale'...\")\n",
    "df_base_features = pl.scan_parquet(PATH_UL).select(\n",
    "    \"siren\",\n",
    "    \"dateCreationUniteLegale\",\n",
    "    \"categorieJuridiqueUniteLegale\",\n",
    "    \"trancheEffectifsUniteLegale\",\n",
    "    \"activitePrincipaleUniteLegale\",\n",
    "    \"categorieEntreprise\",                 # <-- AJOUTÉ\n",
    "    \"economieSocialeSolidaireUniteLegale\", # <-- AJOUTÉ\n",
    "    \"societeMissionUniteLegale\"            # <-- AJOUTÉ\n",
    ")\n",
    "\n",
    "# ===================================================================\n",
    "# ÉTAPE 2: Trouver le SIRET du Siège (HQ) - Fichier 'StockEtablissement'\n",
    "# (On prend 3 features au lieu d'une)\n",
    "# ===================================================================\n",
    "print(\"Étape 2: Lecture de 'StockEtablissement' pour trouver les sièges...\")\n",
    "df_sieges = pl.scan_parquet(PATH_ETAB).filter(\n",
    "    pl.col(\"etablissementSiege\") == True\n",
    ").select(\n",
    "    \"siren\", \n",
    "    \"siret\",\n",
    "    pl.col(\"codePostalEtablissement\").str.slice(0, 2).alias(\"departement\"),\n",
    "    pl.col(\"trancheEffectifsEtablissement\").alias(\"trancheEffectifsSiege\"), # <-- NOUVELLE FEATURE\n",
    "    pl.col(\"caractereEmployeurEtablissement\").alias(\"caractereEmployeurSiege\") # <-- NOUVELLE FEATURE\n",
    ")\n",
    "\n",
    "# ===================================================================\n",
    "# ÉTAPE 3: Trouver la Date de Fermeture (La Cible Y) - Fichier 'StockEtablissementHistorique'\n",
    "# (Rien ne change ici)\n",
    "# ===================================================================\n",
    "print(\"Étape 3: Lecture de 'StockEtablissementHistorique' pour trouver les 'morts'...\")\n",
    "df_fermetures = pl.scan_parquet(PATH_ETAB_HISTO).filter(\n",
    "    pl.col(\"etatAdministratifEtablissement\") == 'F'\n",
    ").select(\n",
    "    \"siret\",\n",
    "    pl.col(\"dateFin\").alias(\"dateFermeture\")\n",
    ").group_by(\"siret\").agg(\n",
    "    pl.col(\"dateFermeture\").max() \n",
    ")\n",
    "\n",
    "# ===================================================================\n",
    "# ÉTAPE 4: Le \"Grand Mariage\" SIRENE\n",
    "# (Rien ne change ici)\n",
    "# ===================================================================\n",
    "print(\"Étape 4: Jointure finale des 3 tables...\")\n",
    "# On utilise .collect() ici pour forcer la 1ère jointure\n",
    "df_master = df_base_features.collect().join(\n",
    "    df_sieges.collect(), on=\"siren\", how=\"left\"\n",
    ")\n",
    "# On joint le 2e\n",
    "df_master = df_master.join(\n",
    "    df_fermetures.collect(), on=\"siret\", how=\"left\"\n",
    ")\n",
    "\n",
    "# ===================================================================\n",
    "# ÉTAPE 5: Sauvegarde\n",
    "# (On a maintenant 14 colonnes \"propres\" !)\n",
    "# ===================================================================\n",
    "print(f\"Sauvegarde du Master File 'Démo Monstrueuse' dans {PATH_OUTPUT}...\")\n",
    "df_final = df_master.select(\n",
    "    # Les Clés (pour la Target)\n",
    "    \"siren\",\n",
    "    \"dateCreationUniteLegale\",\n",
    "    \"dateFermeture\",\n",
    "    # Les 12 Features \"Monstrueuses\"\n",
    "    \"categorieJuridiqueUniteLegale\",\n",
    "    \"trancheEffectifsUniteLegale\",\n",
    "    \"activitePrincipaleUniteLegale\",\n",
    "    \"categorieEntreprise\",\n",
    "    \"economieSocialeSolidaireUniteLegale\",\n",
    "    # \"societeMissionUniteLegale\",\n",
    "    # \"sexeUniteLegale\", # On l'enlève, il est souvent 'null'\n",
    "    pl.col(\"dateCreationUniteLegale\").dt.year().alias(\"anneeCreation\"),\n",
    "    pl.col(\"dateCreationUniteLegale\").dt.month().alias(\"moisCreation\"),\n",
    "    \"departement\",\n",
    "    \"trancheEffectifsSiege\",\n",
    "    \"caractereEmployeurSiege\"\n",
    ")\n",
    "\n",
    "# On s'assure que le dossier 'processed' existe\n",
    "os.makedirs(os.path.dirname(PATH_OUTPUT), exist_ok=True)\n",
    "\n",
    "# On sauvegarde\n",
    "df_final.write_parquet(PATH_OUTPUT)\n",
    "print(f\"--- Script 01 (Démo Monstrueuse) Terminé avec Succès ---\")\n",
    "print(f\"Fichier créé : {PATH_OUTPUT}\")\n",
    "print(f\"Shape finale : {df_final.shape}\")\n",
    "print(df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 14)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>siren</th><th>dateCreationUniteLegale</th><th>dateFermeture</th><th>categorieJuridiqueUniteLegale</th><th>trancheEffectifsUniteLegale</th><th>activitePrincipaleUniteLegale</th><th>categorieEntreprise</th><th>economieSocialeSolidaireUniteLegale</th><th>anneeCreation</th><th>moisCreation</th><th>departement</th><th>trancheEffectifsSiege</th><th>caractereEmployeurSiege</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>&quot;28882409&quot;</td><td>&quot;27710121&quot;</td><td>&quot;1198759&quot;</td><td>2.8882409e7</td><td>&quot;28882409&quot;</td><td>&quot;28860884&quot;</td><td>&quot;10546073&quot;</td><td>&quot;6167001&quot;</td><td>2.7710121e7</td><td>2.7710121e7</td><td>&quot;28560617&quot;</td><td>&quot;28860588&quot;</td><td>&quot;28707092&quot;</td></tr><tr><td>&quot;null_count&quot;</td><td>&quot;0&quot;</td><td>&quot;1172288&quot;</td><td>&quot;27683650&quot;</td><td>0.0</td><td>&quot;0&quot;</td><td>&quot;21525&quot;</td><td>&quot;18336336&quot;</td><td>&quot;22715408&quot;</td><td>1.172288e6</td><td>1.172288e6</td><td>&quot;321792&quot;</td><td>&quot;21821&quot;</td><td>&quot;175317&quot;</td></tr><tr><td>&quot;mean&quot;</td><td>null</td><td>&quot;2005-10-22 13:11:56.004134&quot;</td><td>&quot;2010-09-25 00:21:57.306314&quot;</td><td>3246.573881</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2005.370016</td><td>5.966385</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;std&quot;</td><td>null</td><td>null</td><td>null</td><td>2736.41353</td><td>null</td><td>null</td><td>null</td><td>null</td><td>18.148138</td><td>3.714184</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;min&quot;</td><td>&quot;000325175&quot;</td><td>&quot;0001-01-16&quot;</td><td>&quot;1900-12-31&quot;</td><td>1000.0</td><td>&quot;00&quot;</td><td>&quot;00.00&quot;</td><td>&quot;ETI&quot;</td><td>&quot;N&quot;</td><td>1.0</td><td>1.0</td><td>&quot; D&quot;</td><td>&quot;00&quot;</td><td>&quot;N&quot;</td></tr><tr><td>&quot;25%&quot;</td><td>null</td><td>&quot;1995-01-19&quot;</td><td>&quot;2003-12-24&quot;</td><td>1000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1995.0</td><td>2.0</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;50%&quot;</td><td>null</td><td>&quot;2010-05-01&quot;</td><td>&quot;2010-10-13&quot;</td><td>1000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2010.0</td><td>6.0</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;75%&quot;</td><td>null</td><td>&quot;2019-11-02&quot;</td><td>&quot;2019-12-02&quot;</td><td>5599.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2019.0</td><td>9.0</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;max&quot;</td><td>&quot;999992357&quot;</td><td>&quot;3023-01-06&quot;</td><td>&quot;5015-04-04&quot;</td><td>9970.0</td><td>&quot;NN&quot;</td><td>&quot;99.0Z&quot;</td><td>&quot;PME&quot;</td><td>&quot;O&quot;</td><td>3023.0</td><td>12.0</td><td>&quot;sw&quot;</td><td>&quot;NN&quot;</td><td>&quot;O&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 14)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ statistic ┆ siren     ┆ dateCreat ┆ dateFerme ┆ … ┆ moisCreat ┆ departeme ┆ trancheEf ┆ caracter │\n",
       "│ ---       ┆ ---       ┆ ionUniteL ┆ ture      ┆   ┆ ion       ┆ nt        ┆ fectifsSi ┆ eEmploye │\n",
       "│ str       ┆ str       ┆ egale     ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ege       ┆ urSiege  │\n",
       "│           ┆           ┆ ---       ┆ str       ┆   ┆ f64       ┆ str       ┆ ---       ┆ ---      │\n",
       "│           ┆           ┆ str       ┆           ┆   ┆           ┆           ┆ str       ┆ str      │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ count     ┆ 28882409  ┆ 27710121  ┆ 1198759   ┆ … ┆ 2.7710121 ┆ 28560617  ┆ 28860588  ┆ 28707092 │\n",
       "│           ┆           ┆           ┆           ┆   ┆ e7        ┆           ┆           ┆          │\n",
       "│ null_coun ┆ 0         ┆ 1172288   ┆ 27683650  ┆ … ┆ 1.172288e ┆ 321792    ┆ 21821     ┆ 175317   │\n",
       "│ t         ┆           ┆           ┆           ┆   ┆ 6         ┆           ┆           ┆          │\n",
       "│ mean      ┆ null      ┆ 2005-10-2 ┆ 2010-09-2 ┆ … ┆ 5.966385  ┆ null      ┆ null      ┆ null     │\n",
       "│           ┆           ┆ 2 13:11:5 ┆ 5 00:21:5 ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ 6.004134  ┆ 7.306314  ┆   ┆           ┆           ┆           ┆          │\n",
       "│ std       ┆ null      ┆ null      ┆ null      ┆ … ┆ 3.714184  ┆ null      ┆ null      ┆ null     │\n",
       "│ min       ┆ 000325175 ┆ 0001-01-1 ┆ 1900-12-3 ┆ … ┆ 1.0       ┆  D        ┆ 00        ┆ N        │\n",
       "│           ┆           ┆ 6         ┆ 1         ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 25%       ┆ null      ┆ 1995-01-1 ┆ 2003-12-2 ┆ … ┆ 2.0       ┆ null      ┆ null      ┆ null     │\n",
       "│           ┆           ┆ 9         ┆ 4         ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 50%       ┆ null      ┆ 2010-05-0 ┆ 2010-10-1 ┆ … ┆ 6.0       ┆ null      ┆ null      ┆ null     │\n",
       "│           ┆           ┆ 1         ┆ 3         ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 75%       ┆ null      ┆ 2019-11-0 ┆ 2019-12-0 ┆ … ┆ 9.0       ┆ null      ┆ null      ┆ null     │\n",
       "│           ┆           ┆ 2         ┆ 2         ┆   ┆           ┆           ┆           ┆          │\n",
       "│ max       ┆ 999992357 ┆ 3023-01-0 ┆ 5015-04-0 ┆ … ┆ 12.0      ┆ sw        ┆ NN        ┆ O        │\n",
       "│           ┆           ┆ 6         ┆ 4         ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# je veux clean toutes les nulls dans df_final des colonnes departement, dateCreationUniteLegale, activitéPrincipaleUniteLegale, trancheEffectifsUniteLegale\n",
    "\n",
    "df_clean = df_final.drop_nulls(subset=[\n",
    "    \"departement\", \n",
    "    \"dateCreationUniteLegale\", \n",
    "    \"activitePrincipaleUniteLegale\", \n",
    "    \"trancheEffectifsUniteLegale\",\n",
    "    \"categorieEntreprise\",\n",
    "    \"caractereEmployeurSiege\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# je veux que tu m'enleve toutes les lignes ou la date de fermeture est avant la date de création masi aussi ou la date de fermeture est le même jour que la date de création mais aussi si la date de fermeture est plus tard que la date du jour \n",
    "\n",
    "from datetime import datetime\n",
    "today = datetime.today().date()\n",
    "\n",
    "df_clean = df_clean.filter(\n",
    "    ( (pl.col(\"dateFermeture\").is_null()) | \n",
    "      (pl.col(\"dateFermeture\") > pl.col(\"dateCreationUniteLegale\")) & \n",
    "      (pl.col(\"dateFermeture\") < pl.lit(today)) )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Je veux enlever les entreprises crées avant 1970 et après la date d'ajourd'hui\n",
    "\n",
    "df_clean = df_clean.filter(\n",
    "    (pl.col(\"dateCreationUniteLegale\").dt.year() >= 1970) & \n",
    "    (pl.col(\"dateCreationUniteLegale\") <= pl.lit(today))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 14)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>siren</th><th>dateCreationUniteLegale</th><th>dateFermeture</th><th>categorieJuridiqueUniteLegale</th><th>trancheEffectifsUniteLegale</th><th>activitePrincipaleUniteLegale</th><th>categorieEntreprise</th><th>economieSocialeSolidaireUniteLegale</th><th>anneeCreation</th><th>moisCreation</th><th>departement</th><th>trancheEffectifsSiege</th><th>caractereEmployeurSiege</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>&quot;10223432&quot;</td><td>&quot;10223432&quot;</td><td>&quot;432317&quot;</td><td>1.0223432e7</td><td>&quot;10223432&quot;</td><td>&quot;10223432&quot;</td><td>&quot;10223432&quot;</td><td>&quot;2902459&quot;</td><td>1.0223432e7</td><td>1.0223432e7</td><td>&quot;10223432&quot;</td><td>&quot;10223432&quot;</td><td>&quot;10223432&quot;</td></tr><tr><td>&quot;null_count&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;9791115&quot;</td><td>0.0</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;7320973&quot;</td><td>0.0</td><td>0.0</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td></tr><tr><td>&quot;mean&quot;</td><td>null</td><td>&quot;2012-06-08 22:08:05.052299&quot;</td><td>&quot;2018-11-30 18:59:23.235311&quot;</td><td>3056.155026</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2011.980528</td><td>6.185161</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;std&quot;</td><td>null</td><td>null</td><td>null</td><td>2549.956636</td><td>null</td><td>null</td><td>null</td><td>null</td><td>11.055441</td><td>3.593599</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;min&quot;</td><td>&quot;000325175&quot;</td><td>&quot;1970-01-01&quot;</td><td>&quot;1971-04-19&quot;</td><td>1000.0</td><td>&quot;00&quot;</td><td>&quot;00.00Z&quot;</td><td>&quot;ETI&quot;</td><td>&quot;N&quot;</td><td>1970.0</td><td>1.0</td><td>&quot; D&quot;</td><td>&quot;00&quot;</td><td>&quot;N&quot;</td></tr><tr><td>&quot;25%&quot;</td><td>null</td><td>&quot;2007-05-10&quot;</td><td>&quot;2017-01-02&quot;</td><td>1000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2007.0</td><td>3.0</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;50%&quot;</td><td>null</td><td>&quot;2016-02-01&quot;</td><td>&quot;2020-12-31&quot;</td><td>1000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2016.0</td><td>6.0</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;75%&quot;</td><td>null</td><td>&quot;2021-02-11&quot;</td><td>&quot;2022-12-31&quot;</td><td>5499.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2021.0</td><td>9.0</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;max&quot;</td><td>&quot;999992357&quot;</td><td>&quot;2025-11-01&quot;</td><td>&quot;2025-11-02&quot;</td><td>9970.0</td><td>&quot;NN&quot;</td><td>&quot;99.00Z&quot;</td><td>&quot;PME&quot;</td><td>&quot;O&quot;</td><td>2025.0</td><td>12.0</td><td>&quot;sw&quot;</td><td>&quot;NN&quot;</td><td>&quot;O&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 14)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ statistic ┆ siren     ┆ dateCreat ┆ dateFerme ┆ … ┆ moisCreat ┆ departeme ┆ trancheEf ┆ caracter │\n",
       "│ ---       ┆ ---       ┆ ionUniteL ┆ ture      ┆   ┆ ion       ┆ nt        ┆ fectifsSi ┆ eEmploye │\n",
       "│ str       ┆ str       ┆ egale     ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ege       ┆ urSiege  │\n",
       "│           ┆           ┆ ---       ┆ str       ┆   ┆ f64       ┆ str       ┆ ---       ┆ ---      │\n",
       "│           ┆           ┆ str       ┆           ┆   ┆           ┆           ┆ str       ┆ str      │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ count     ┆ 10223432  ┆ 10223432  ┆ 432317    ┆ … ┆ 1.0223432 ┆ 10223432  ┆ 10223432  ┆ 10223432 │\n",
       "│           ┆           ┆           ┆           ┆   ┆ e7        ┆           ┆           ┆          │\n",
       "│ null_coun ┆ 0         ┆ 0         ┆ 9791115   ┆ … ┆ 0.0       ┆ 0         ┆ 0         ┆ 0        │\n",
       "│ t         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ mean      ┆ null      ┆ 2012-06-0 ┆ 2018-11-3 ┆ … ┆ 6.185161  ┆ null      ┆ null      ┆ null     │\n",
       "│           ┆           ┆ 8 22:08:0 ┆ 0 18:59:2 ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ 5.052299  ┆ 3.235311  ┆   ┆           ┆           ┆           ┆          │\n",
       "│ std       ┆ null      ┆ null      ┆ null      ┆ … ┆ 3.593599  ┆ null      ┆ null      ┆ null     │\n",
       "│ min       ┆ 000325175 ┆ 1970-01-0 ┆ 1971-04-1 ┆ … ┆ 1.0       ┆  D        ┆ 00        ┆ N        │\n",
       "│           ┆           ┆ 1         ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 25%       ┆ null      ┆ 2007-05-1 ┆ 2017-01-0 ┆ … ┆ 3.0       ┆ null      ┆ null      ┆ null     │\n",
       "│           ┆           ┆ 0         ┆ 2         ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 50%       ┆ null      ┆ 2016-02-0 ┆ 2020-12-3 ┆ … ┆ 6.0       ┆ null      ┆ null      ┆ null     │\n",
       "│           ┆           ┆ 1         ┆ 1         ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 75%       ┆ null      ┆ 2021-02-1 ┆ 2022-12-3 ┆ … ┆ 9.0       ┆ null      ┆ null      ┆ null     │\n",
       "│           ┆           ┆ 1         ┆ 1         ┆   ┆           ┆           ┆           ┆          │\n",
       "│ max       ┆ 999992357 ┆ 2025-11-0 ┆ 2025-11-0 ┆ … ┆ 12.0      ┆ sw        ┆ NN        ┆ O        │\n",
       "│           ┆           ┆ 1         ┆ 2         ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 13)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>siren</th><th>dateCreationUniteLegale</th><th>dateFermeture</th><th>categorieJuridiqueUniteLegale</th><th>trancheEffectifsUniteLegale</th><th>activitePrincipaleUniteLegale</th><th>categorieEntreprise</th><th>economieSocialeSolidaireUniteLegale</th><th>anneeCreation</th><th>moisCreation</th><th>departement</th><th>trancheEffectifsSiege</th><th>caractereEmployeurSiege</th></tr><tr><td>str</td><td>date</td><td>date</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i32</td><td>i8</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;000325175&quot;</td><td>2000-09-26</td><td>null</td><td>1000</td><td>&quot;NN&quot;</td><td>&quot;32.12Z&quot;</td><td>&quot;PME&quot;</td><td>null</td><td>2000</td><td>9</td><td>&quot;13&quot;</td><td>&quot;NN&quot;</td><td>&quot;N&quot;</td></tr><tr><td>&quot;005540273&quot;</td><td>1972-01-01</td><td>null</td><td>1000</td><td>&quot;NN&quot;</td><td>&quot;68.20B&quot;</td><td>&quot;PME&quot;</td><td>null</td><td>1972</td><td>1</td><td>&quot;04&quot;</td><td>&quot;NN&quot;</td><td>&quot;N&quot;</td></tr><tr><td>&quot;005541552&quot;</td><td>1974-01-01</td><td>null</td><td>5710</td><td>&quot;02&quot;</td><td>&quot;56.10C&quot;</td><td>&quot;PME&quot;</td><td>&quot;N&quot;</td><td>1974</td><td>1</td><td>&quot;04&quot;</td><td>&quot;01&quot;</td><td>&quot;O&quot;</td></tr><tr><td>&quot;005641154&quot;</td><td>1981-02-01</td><td>null</td><td>1000</td><td>&quot;NN&quot;</td><td>&quot;68.31Z&quot;</td><td>&quot;PME&quot;</td><td>null</td><td>1981</td><td>2</td><td>&quot;04&quot;</td><td>&quot;NN&quot;</td><td>&quot;N&quot;</td></tr><tr><td>&quot;005742580&quot;</td><td>1993-03-01</td><td>null</td><td>1000</td><td>&quot;NN&quot;</td><td>&quot;68.20B&quot;</td><td>&quot;PME&quot;</td><td>null</td><td>1993</td><td>3</td><td>&quot;04&quot;</td><td>&quot;NN&quot;</td><td>&quot;N&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 13)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ siren     ┆ dateCreat ┆ dateFerme ┆ categorie ┆ … ┆ moisCreat ┆ departeme ┆ trancheEf ┆ caracter │\n",
       "│ ---       ┆ ionUniteL ┆ ture      ┆ Juridique ┆   ┆ ion       ┆ nt        ┆ fectifsSi ┆ eEmploye │\n",
       "│ str       ┆ egale     ┆ ---       ┆ UniteLega ┆   ┆ ---       ┆ ---       ┆ ege       ┆ urSiege  │\n",
       "│           ┆ ---       ┆ date      ┆ le        ┆   ┆ i8        ┆ str       ┆ ---       ┆ ---      │\n",
       "│           ┆ date      ┆           ┆ ---       ┆   ┆           ┆           ┆ str       ┆ str      │\n",
       "│           ┆           ┆           ┆ i64       ┆   ┆           ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 000325175 ┆ 2000-09-2 ┆ null      ┆ 1000      ┆ … ┆ 9         ┆ 13        ┆ NN        ┆ N        │\n",
       "│           ┆ 6         ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 005540273 ┆ 1972-01-0 ┆ null      ┆ 1000      ┆ … ┆ 1         ┆ 04        ┆ NN        ┆ N        │\n",
       "│           ┆ 1         ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 005541552 ┆ 1974-01-0 ┆ null      ┆ 5710      ┆ … ┆ 1         ┆ 04        ┆ 01        ┆ O        │\n",
       "│           ┆ 1         ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 005641154 ┆ 1981-02-0 ┆ null      ┆ 1000      ┆ … ┆ 2         ┆ 04        ┆ NN        ┆ N        │\n",
       "│           ┆ 1         ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 005742580 ┆ 1993-03-0 ┆ null      ┆ 1000      ┆ … ┆ 3         ┆ 04        ┆ NN        ┆ N        │\n",
       "│           ┆ 1         ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'Démo Monstrueuse' chargé. Shape: (28882409, 13)\n",
      "Filtrage de la Cohorte 2018...\n",
      "Cohorte 2018 isolée. Shape: (845851, 13)\n",
      "Création de la Cible (is_failed_in_3y)...\n",
      "--- DATASET ML PRÊT ---\n",
      "Shape finale prête pour le ML : (845851, 15)\n",
      "\n",
      "Répartition de la Cible (Y) :\n",
      "shape: (2, 2)\n",
      "┌─────────────────┬────────┐\n",
      "│ is_failed_in_3y ┆ count  │\n",
      "│ ---             ┆ ---    │\n",
      "│ i32             ┆ u32    │\n",
      "╞═════════════════╪════════╡\n",
      "│ 0               ┆ 833982 │\n",
      "│ 1               ┆ 11869  │\n",
      "└─────────────────┴────────┘\n",
      "\n",
      "Aperçu du DataFrame final :\n",
      "shape: (5, 15)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ siren     ┆ dateCreat ┆ dateFerme ┆ categorie ┆ … ┆ trancheEf ┆ caractere ┆ date_limi ┆ is_faile │\n",
      "│ ---       ┆ ionUniteL ┆ ture      ┆ Juridique ┆   ┆ fectifsSi ┆ Employeur ┆ te_3_ans  ┆ d_in_3y  │\n",
      "│ str       ┆ egale     ┆ ---       ┆ UniteLega ┆   ┆ ege       ┆ Siege     ┆ ---       ┆ ---      │\n",
      "│           ┆ ---       ┆ date      ┆ le        ┆   ┆ ---       ┆ ---       ┆ date      ┆ i32      │\n",
      "│           ┆ date      ┆           ┆ ---       ┆   ┆ str       ┆ str       ┆           ┆          │\n",
      "│           ┆           ┆           ┆ i64       ┆   ┆           ┆           ┆           ┆          │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│ 130023385 ┆ 2018-01-0 ┆ null      ┆ 7383      ┆ … ┆ 51        ┆ O         ┆ 2021-01-0 ┆ 0        │\n",
      "│           ┆ 1         ┆           ┆           ┆   ┆           ┆           ┆ 1         ┆          │\n",
      "│ 130023583 ┆ 2018-01-0 ┆ null      ┆ 7383      ┆ … ┆ NN        ┆ O         ┆ 2021-01-0 ┆ 0        │\n",
      "│           ┆ 1         ┆           ┆           ┆   ┆           ┆           ┆ 1         ┆          │\n",
      "│ 130023740 ┆ 2018-01-0 ┆ null      ┆ 7381      ┆ … ┆ 32        ┆ O         ┆ 2021-01-0 ┆ 0        │\n",
      "│           ┆ 1         ┆           ┆           ┆   ┆           ┆           ┆ 1         ┆          │\n",
      "│ 130023799 ┆ 2018-01-0 ┆ null      ┆ 7410      ┆ … ┆ 12        ┆ N         ┆ 2021-01-0 ┆ 0        │\n",
      "│           ┆ 1         ┆           ┆           ┆   ┆           ┆           ┆ 1         ┆          │\n",
      "│ 130023807 ┆ 2018-01-0 ┆ null      ┆ 7410      ┆ … ┆ 22        ┆ O         ┆ 2021-01-0 ┆ 0        │\n",
      "│           ┆ 1         ┆           ┆           ┆   ┆           ┆           ┆ 1         ┆          │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- 1. CHARGER LE \"MASTER FILE\" DÉMO MONSTRUEUX ---\n",
    "PATH_ML_DEMO = \"../Data/processed/sirene_infos_MONSTROUS_DEMO.parquet\"\n",
    "try:\n",
    "    df_master = pl.read_parquet(PATH_ML_DEMO)\n",
    "    print(f\"Dataset 'Démo Monstrueuse' chargé. Shape: {df_master.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERREUR: Fichier non trouvé : {PATH_ML_DEMO}\")\n",
    "    print(\"Assure-toi que ton script de Data Prep a bien tourné.\")\n",
    "\n",
    "# --- 2. FILTRER LA COHORTE ---\n",
    "# On va étudier les entreprises créées en 2018\n",
    "print(\"Filtrage de la Cohorte 2018...\")\n",
    "df_ml = df_master.filter(\n",
    "    pl.col(\"dateCreationUniteLegale\").dt.year() == 2018\n",
    ")\n",
    "print(f\"Cohorte 2018 isolée. Shape: {df_ml.shape}\")\n",
    "\n",
    "# --- 3. CRÉATION DE LA TARGET (Y) ---\n",
    "print(\"Création de la Cible (is_failed_in_3y)...\")\n",
    "\n",
    "df_ml = df_ml.with_columns(\n",
    "    # Date limite = 3 ans après la création\n",
    "    (pl.col(\"dateCreationUniteLegale\").dt.offset_by(\"3y\")).alias(\"date_limite_3_ans\")\n",
    ").with_columns(\n",
    "    # 1 = Faillite si Fermé AVANT la date limite\n",
    "    pl.when(\n",
    "        (pl.col(\"dateFermeture\").is_not_null()) & # dateFermeture n'est pas null\n",
    "        (pl.col(\"dateFermeture\") < pl.col(\"date_limite_3_ans\"))\n",
    "    ).then(1)\n",
    "    .otherwise(0)\n",
    "    .alias(\"is_failed_in_3y\")\n",
    ").fill_null(\"INCONNU\") # Nettoyage final des features catégorielles\n",
    "\n",
    "# --- 4. VÉRIFICATION (EDA Rapide) ---\n",
    "print(\"--- DATASET ML PRÊT ---\")\n",
    "print(f\"Shape finale prête pour le ML : {df_ml.shape}\")\n",
    "\n",
    "# On vérifie la répartition de notre nouvelle Cible\n",
    "print(\"\\nRépartition de la Cible (Y) :\")\n",
    "print(df_ml.get_column(\"is_failed_in_3y\").value_counts())\n",
    "\n",
    "print(\"\\nAperçu du DataFrame final :\")\n",
    "print(df_ml.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 16)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>siren</th><th>dateCreationUniteLegale</th><th>dateFermeture</th><th>categorieJuridiqueUniteLegale</th><th>trancheEffectifsUniteLegale</th><th>activitePrincipaleUniteLegale</th><th>categorieEntreprise</th><th>economieSocialeSolidaireUniteLegale</th><th>anneeCreation</th><th>moisCreation</th><th>departement</th><th>trancheEffectifsSiege</th><th>caractereEmployeurSiege</th><th>date_limite_3_ans</th><th>is_failed_in_3y</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>&quot;845851&quot;</td><td>&quot;845851&quot;</td><td>&quot;27598&quot;</td><td>845851.0</td><td>&quot;845851&quot;</td><td>&quot;845851&quot;</td><td>&quot;845851&quot;</td><td>&quot;845851&quot;</td><td>845851.0</td><td>845851.0</td><td>&quot;845851&quot;</td><td>&quot;845851&quot;</td><td>&quot;845851&quot;</td><td>&quot;845851&quot;</td><td>845851.0</td></tr><tr><td>&quot;null_count&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;818253&quot;</td><td>0.0</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>0.0</td><td>0.0</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>null</td><td>&quot;2018-06-19 15:03:25.836252&quot;</td><td>&quot;2021-09-18 01:42:16.096818&quot;</td><td>3199.927141</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2018.0</td><td>6.25518</td><td>null</td><td>null</td><td>null</td><td>&quot;2021-06-19 15:03:25.836252&quot;</td><td>0.014032</td></tr><tr><td>&quot;std&quot;</td><td>null</td><td>null</td><td>null</td><td>2668.588219</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.0</td><td>3.526369</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.117623</td></tr><tr><td>&quot;min&quot;</td><td>&quot;130023385&quot;</td><td>&quot;2018-01-01&quot;</td><td>&quot;2017-12-31&quot;</td><td>1000.0</td><td>&quot;00&quot;</td><td>&quot;00.00Z&quot;</td><td>&quot;ETI&quot;</td><td>&quot;INCONNU&quot;</td><td>2018.0</td><td>1.0</td><td>&quot;01&quot;</td><td>&quot;00&quot;</td><td>&quot;INCONNU&quot;</td><td>&quot;2021-01-01&quot;</td><td>0.0</td></tr><tr><td>&quot;25%&quot;</td><td>null</td><td>&quot;2018-03-15&quot;</td><td>&quot;2020-05-30&quot;</td><td>1000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2018.0</td><td>3.0</td><td>null</td><td>null</td><td>null</td><td>&quot;2021-03-15&quot;</td><td>0.0</td></tr><tr><td>&quot;50%&quot;</td><td>null</td><td>&quot;2018-06-18&quot;</td><td>&quot;2021-11-03&quot;</td><td>1000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2018.0</td><td>6.0</td><td>null</td><td>null</td><td>null</td><td>&quot;2021-06-18&quot;</td><td>0.0</td></tr><tr><td>&quot;75%&quot;</td><td>null</td><td>&quot;2018-09-28&quot;</td><td>&quot;2023-02-05&quot;</td><td>5710.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2018.0</td><td>9.0</td><td>null</td><td>null</td><td>null</td><td>&quot;2021-09-28&quot;</td><td>0.0</td></tr><tr><td>&quot;max&quot;</td><td>&quot;993394725&quot;</td><td>&quot;2018-12-31&quot;</td><td>&quot;2025-10-31&quot;</td><td>9970.0</td><td>&quot;NN&quot;</td><td>&quot;99.00Z&quot;</td><td>&quot;PME&quot;</td><td>&quot;O&quot;</td><td>2018.0</td><td>12.0</td><td>&quot;[N&quot;</td><td>&quot;NN&quot;</td><td>&quot;O&quot;</td><td>&quot;2021-12-31&quot;</td><td>1.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 16)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ statistic ┆ siren     ┆ dateCreat ┆ dateFerme ┆ … ┆ trancheEf ┆ caractere ┆ date_limi ┆ is_faile │\n",
       "│ ---       ┆ ---       ┆ ionUniteL ┆ ture      ┆   ┆ fectifsSi ┆ Employeur ┆ te_3_ans  ┆ d_in_3y  │\n",
       "│ str       ┆ str       ┆ egale     ┆ ---       ┆   ┆ ege       ┆ Siege     ┆ ---       ┆ ---      │\n",
       "│           ┆           ┆ ---       ┆ str       ┆   ┆ ---       ┆ ---       ┆ str       ┆ f64      │\n",
       "│           ┆           ┆ str       ┆           ┆   ┆ str       ┆ str       ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ count     ┆ 845851    ┆ 845851    ┆ 27598     ┆ … ┆ 845851    ┆ 845851    ┆ 845851    ┆ 845851.0 │\n",
       "│ null_coun ┆ 0         ┆ 0         ┆ 818253    ┆ … ┆ 0         ┆ 0         ┆ 0         ┆ 0.0      │\n",
       "│ t         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ mean      ┆ null      ┆ 2018-06-1 ┆ 2021-09-1 ┆ … ┆ null      ┆ null      ┆ 2021-06-1 ┆ 0.014032 │\n",
       "│           ┆           ┆ 9 15:03:2 ┆ 8 01:42:1 ┆   ┆           ┆           ┆ 9 15:03:2 ┆          │\n",
       "│           ┆           ┆ 5.836252  ┆ 6.096818  ┆   ┆           ┆           ┆ 5.836252  ┆          │\n",
       "│ std       ┆ null      ┆ null      ┆ null      ┆ … ┆ null      ┆ null      ┆ null      ┆ 0.117623 │\n",
       "│ min       ┆ 130023385 ┆ 2018-01-0 ┆ 2017-12-3 ┆ … ┆ 00        ┆ INCONNU   ┆ 2021-01-0 ┆ 0.0      │\n",
       "│           ┆           ┆ 1         ┆ 1         ┆   ┆           ┆           ┆ 1         ┆          │\n",
       "│ 25%       ┆ null      ┆ 2018-03-1 ┆ 2020-05-3 ┆ … ┆ null      ┆ null      ┆ 2021-03-1 ┆ 0.0      │\n",
       "│           ┆           ┆ 5         ┆ 0         ┆   ┆           ┆           ┆ 5         ┆          │\n",
       "│ 50%       ┆ null      ┆ 2018-06-1 ┆ 2021-11-0 ┆ … ┆ null      ┆ null      ┆ 2021-06-1 ┆ 0.0      │\n",
       "│           ┆           ┆ 8         ┆ 3         ┆   ┆           ┆           ┆ 8         ┆          │\n",
       "│ 75%       ┆ null      ┆ 2018-09-2 ┆ 2023-02-0 ┆ … ┆ null      ┆ null      ┆ 2021-09-2 ┆ 0.0      │\n",
       "│           ┆           ┆ 8         ┆ 5         ┆   ┆           ┆           ┆ 8         ┆          │\n",
       "│ max       ┆ 993394725 ┆ 2018-12-3 ┆ 2025-10-3 ┆ … ┆ NN        ┆ O         ┆ 2021-12-3 ┆ 1.0      │\n",
       "│           ┆           ┆ 1         ┆ 1         ┆   ┆           ┆           ┆ 1         ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ml.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvergarde ça dans un parquet\n",
    "\n",
    "PATH_OUTPUT_CLEAN = \"../Data/processed/sirene_infos_CLEAN.parquet\"\n",
    "\n",
    "df_ml.write_parquet(PATH_OUTPUT_CLEAN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Schema([('siren', String),\n",
       "        ('dateCreationUniteLegale', Date),\n",
       "        ('dateFermeture', Date),\n",
       "        ('categorieJuridiqueUniteLegale', Int64),\n",
       "        ('trancheEffectifsUniteLegale', String),\n",
       "        ('activitePrincipaleUniteLegale', String),\n",
       "        ('categorieEntreprise', String),\n",
       "        ('economieSocialeSolidaireUniteLegale', String),\n",
       "        ('anneeCreation', Int32),\n",
       "        ('moisCreation', Int8),\n",
       "        ('departement', String),\n",
       "        ('trancheEffectifsSiege', String),\n",
       "        ('caractereEmployeurSiege', String),\n",
       "        ('date_limite_3_ans', Date),\n",
       "        ('is_failed_in_3y', Int32)])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ml.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 'df_ml' (Cohorte 2018) chargé. Shape: (465989, 15)\n",
      "Définition des features pour le Modèle A+ (Démo Étendue)...\n",
      "Features (X) sélectionnées: ['categorieJuridiqueUniteLegale', 'trancheEffectifsUniteLegale', 'activitePrincipaleUniteLegale', 'categorieEntreprise', 'economieSocialeSolidaireUniteLegale', 'moisCreation', 'departement', 'trancheEffectifsSiege', 'caractereEmployeurSiege']\n",
      "Target (y) sélectionnée: is_failed_in_3y\n",
      "Ratio de déséquilibre : 60.21\n",
      "Preprocessing avec OneHotEncoder...\n",
      "Création de la pipeline (Preprocessor + XGBoost)...\n",
      "Lancement de la K-Fold Cross-Validation (k=5) (AUC, Accuracy, F1)...\n",
      "---\n",
      "--- RÉSULTATS DU MODÈLE A+ (ITÉRATION 'DÉMO MONSTRUEUSE') ---\n",
      "Score ROC-AUC MOYEN (CV) : 0.7724 (+/- 0.0029)\n",
      "Score Accuracy MOYEN (CV) : 0.6133 (+/- 0.0041)\n",
      "Score F1-Macro MOYEN (CV) : 0.4102 (+/- 0.0024)\n",
      "---\n",
      "Score Baseline (Modèle A, 4 features): 0.7619 AUC, 0.40 F1-Macro\n",
      "Score actuel (Modèle A+, 9 features): 0.7724 AUC, 0.4102 F1-Macro\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, cross_validate # <-- ON CHANGE L'IMPORT\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, f1_score, accuracy_score # <-- ON IMPORTE LES SCORES\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# df_ml est ton DataFrame de la cellule précédente\n",
    "if 'df_ml' not in locals():\n",
    "    print(\"ERREUR: 'df_ml' n'est pas chargé. Exécute la cellule 'Création de la Cohorte' d'abord.\")\n",
    "    raise NameError(\"df_ml is not defined\")\n",
    "else:\n",
    "    print(f\"DataFrame 'df_ml' (Cohorte 2018) chargé. Shape: {df_ml.shape}\")\n",
    "\n",
    "\n",
    "# --- 1. DÉFINITION DES FEATURES (ITÉRATION J : \"DÉMO MONSTRUEUSE\") ---\n",
    "print(\"Définition des features pour le Modèle A+ (Démo Étendue)...\")\n",
    "\n",
    "# (On garde la liste des 9 features qui marchent)\n",
    "DEMO_FEATURES_PLUS = [\n",
    "    \"categorieJuridiqueUniteLegale\",\n",
    "    \"trancheEffectifsUniteLegale\",\n",
    "    \"activitePrincipaleUniteLegale\",\n",
    "    \"categorieEntreprise\",\n",
    "    \"economieSocialeSolidaireUniteLegale\",\n",
    "    # \"societeMissionUniteLegale\", # On l'a oubliée dans le script 01, on la laisse de côté\n",
    "    \"moisCreation\",\n",
    "    \"departement\",\n",
    "    \"trancheEffectifsSiege\",\n",
    "    \"caractereEmployeurSiege\"\n",
    "]\n",
    "\n",
    "TARGET = \"is_failed_in_3y\"\n",
    "\n",
    "X = df_ml.select(DEMO_FEATURES_PLUS).fill_null(\"INCONNU\").to_pandas()\n",
    "y = df_ml.select(TARGET).to_pandas().squeeze()\n",
    "\n",
    "print(f\"Features (X) sélectionnées: {X.columns.to_list()}\")\n",
    "print(f\"Target (y) sélectionnée: {y.name}\")\n",
    "\n",
    "# --- 2. GESTION DU DÉSÉQUILIBRE ---\n",
    "scale_pos_weight = y.value_counts()[0] / y.value_counts()[1]\n",
    "print(f\"Ratio de déséquilibre : {scale_pos_weight:.2f}\")\n",
    "\n",
    "# --- 3. PRÉPARATION (Preprocessing) ---\n",
    "print(\"Preprocessing avec OneHotEncoder...\")\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[(\"cat\", categorical_transformer, DEMO_FEATURES_PLUS)],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "# --- 4. CRÉATION DE LA PIPELINE DE MODÉLISATION ---\n",
    "print(\"Création de la pipeline (Preprocessor + XGBoost)...\")\n",
    "model_A_plus = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', XGBClassifier(\n",
    "        scale_pos_weight=scale_pos_weight, \n",
    "        use_label_encoder=False, \n",
    "        eval_metric='logloss',\n",
    "        random_state=42,\n",
    "        enable_categorical=False \n",
    "    ))\n",
    "])\n",
    "\n",
    "# --- 5. ENTRAÎNEMENT & ÉVALUATION (CROSS-VALIDATION \"MONSTRUEUSE\") ---\n",
    "# ▼▼▼ ON CHANGE TOUTE CETTE PARTIE ▼▼▼\n",
    "print(\"Lancement de la K-Fold Cross-Validation (k=5) (AUC, Accuracy, F1)...\")\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# On définit les 3 scores qu'on veut\n",
    "scoring_metrics = {\n",
    "    'roc_auc': 'roc_auc',\n",
    "    'accuracy': 'accuracy',\n",
    "    'f1_macro': make_scorer(f1_score, average='macro') # On prend f1_macro pour le déséquilibre\n",
    "}\n",
    "\n",
    "# On utilise 'cross_validate' (pas 'cross_val_score')\n",
    "scores = cross_validate(\n",
    "    model_A_plus, \n",
    "    X, y, \n",
    "    cv=kfold, \n",
    "    scoring=scoring_metrics, \n",
    "    n_jobs=-1\n",
    ")\n",
    "# ▲▲▲ ON CHANGE TOUTE CETTE PARTIE ▲▲▲\n",
    "\n",
    "# --- 6. RÉSULTATS (PLUS COMPLETS) ---\n",
    "print(\"---\")\n",
    "print(\"--- RÉSULTATS DU MODÈLE A+ (ITÉRATION 'DÉMO MONSTRUEUSE') ---\")\n",
    "print(f\"Score ROC-AUC MOYEN (CV) : {np.mean(scores['test_roc_auc']):.4f} (+/- {np.std(scores['test_roc_auc']):.4f})\")\n",
    "print(f\"Score Accuracy MOYEN (CV) : {np.mean(scores['test_accuracy']):.4f} (+/- {np.std(scores['test_accuracy']):.4f})\")\n",
    "print(f\"Score F1-Macro MOYEN (CV) : {np.mean(scores['test_f1_macro']):.4f} (+/- {np.std(scores['test_f1_macro']):.4f})\")\n",
    "print(\"---\")\n",
    "print(f\"Score Baseline (Modèle A, 4 features): 0.7619 AUC, 0.40 F1-Macro\")\n",
    "print(f\"Score actuel (Modèle A+, {len(DEMO_FEATURES_PLUS)} features): {np.mean(scores['test_roc_auc']):.4f} AUC, {np.mean(scores['test_f1_macro']):.4f} F1-Macro\")\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ugo/Documents/Cours/Master/Cours/M1/Supervised Learning/Final Project/.venv/lib/python3.13/site-packages/mlflow/tracking/_tracking_service/utils.py:140: FutureWarning: Filesystem tracking backend (e.g., './mlruns') is deprecated. Please switch to a database backend (e.g., 'sqlite:///mlflow.db'). For feedback, see: https://github.com/mlflow/mlflow/issues/18534\n",
      "  return FileStore(store_uri, store_uri)\n",
      "2025/11/17 18:11:15 INFO mlflow.tracking.fluent: Experiment with name 'Projet_SIRENE_Classification_Finale' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 'df_ml' (Cohorte 2018) chargé. Shape: (465989, 15)\n",
      "Features (X) sélectionnées: 9\n",
      "Preprocessing avec OneHotEncoder...\n",
      "Création de la pipeline (Preprocessor + SMOTE + XGBoost)...\n",
      "Lancement du 'Monstrous' Tuning (RandomizedSearchCV)...\n",
      "Lancement de l'entraînement (peut prendre 10-20 minutes)...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=10, classifier__n_estimators=100, classifier__subsample=0.7; total time= 1.2min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=10, classifier__n_estimators=100, classifier__subsample=0.7; total time= 1.4min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=3, classifier__n_estimators=300, classifier__subsample=0.7; total time= 1.9min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=3, classifier__n_estimators=300, classifier__subsample=0.7; total time= 2.0min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__max_depth=7, classifier__n_estimators=200, classifier__subsample=0.7; total time= 2.1min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=3, classifier__n_estimators=300, classifier__subsample=0.7; total time= 2.1min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__max_depth=7, classifier__n_estimators=200, classifier__subsample=0.7; total time= 2.1min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__max_depth=7, classifier__n_estimators=200, classifier__subsample=0.7; total time= 2.2min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=10, classifier__n_estimators=100, classifier__subsample=0.7; total time=  57.1s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=10, classifier__n_estimators=100, classifier__subsample=0.7; total time= 1.2min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=3, classifier__n_estimators=300, classifier__subsample=0.7; total time= 2.5min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__max_depth=7, classifier__n_estimators=200, classifier__subsample=0.7; total time= 2.6min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=3, classifier__n_estimators=300, classifier__subsample=0.7; total time= 2.9min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=3, classifier__n_estimators=100, classifier__subsample=0.7; total time=  56.8s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__max_depth=7, classifier__n_estimators=200, classifier__subsample=0.7; total time= 3.0min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=3, classifier__n_estimators=100, classifier__subsample=0.7; total time= 1.1min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=3, classifier__n_estimators=100, classifier__subsample=0.7; total time= 1.1min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=3, classifier__n_estimators=100, classifier__subsample=0.7; total time= 1.2min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=10, classifier__n_estimators=100, classifier__subsample=0.7; total time= 1.5min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=3, classifier__n_estimators=100, classifier__subsample=0.7; total time= 1.3min\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=3, classifier__n_estimators=300, classifier__subsample=0.7; total time= 1.9min\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=3, classifier__n_estimators=300, classifier__subsample=0.7; total time= 1.8min\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=3, classifier__n_estimators=300, classifier__subsample=0.7; total time= 2.0min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__max_depth=3, classifier__n_estimators=200, classifier__subsample=0.7; total time= 1.5min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__max_depth=3, classifier__n_estimators=200, classifier__subsample=0.7; total time= 1.5min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__max_depth=3, classifier__n_estimators=200, classifier__subsample=0.7; total time= 1.5min\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=3, classifier__n_estimators=300, classifier__subsample=0.7; total time= 2.4min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__max_depth=3, classifier__n_estimators=200, classifier__subsample=0.7; total time= 1.9min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=5, classifier__n_estimators=300, classifier__subsample=0.7; total time= 1.8min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=5, classifier__n_estimators=300, classifier__subsample=0.7; total time= 1.8min\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=3, classifier__n_estimators=300, classifier__subsample=0.7; total time= 2.6min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__max_depth=3, classifier__n_estimators=200, classifier__subsample=0.7; total time= 2.2min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=7, classifier__n_estimators=100, classifier__subsample=0.7; total time=  49.2s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=5, classifier__n_estimators=300, classifier__subsample=0.7; total time= 1.8min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=7, classifier__n_estimators=100, classifier__subsample=0.7; total time=  58.7s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=7, classifier__n_estimators=100, classifier__subsample=0.7; total time=  51.2s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=5, classifier__n_estimators=300, classifier__subsample=0.7; total time= 2.3min\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=5, classifier__n_estimators=300, classifier__subsample=0.7; total time= 2.0min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=7, classifier__n_estimators=100, classifier__subsample=0.7; total time= 1.2min\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=5, classifier__n_estimators=300, classifier__subsample=0.7; total time= 2.2min\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=5, classifier__n_estimators=300, classifier__subsample=0.7; total time= 1.9min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__max_depth=5, classifier__n_estimators=100, classifier__subsample=0.7; total time=  49.0s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=5, classifier__n_estimators=300, classifier__subsample=0.7; total time= 2.4min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=7, classifier__n_estimators=100, classifier__subsample=0.7; total time= 1.1min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=5, classifier__n_estimators=300, classifier__subsample=0.7; total time= 2.7min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__max_depth=5, classifier__n_estimators=100, classifier__subsample=0.7; total time=  57.0s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__max_depth=5, classifier__n_estimators=100, classifier__subsample=0.7; total time=  53.3s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__max_depth=5, classifier__n_estimators=100, classifier__subsample=0.7; total time= 1.1min\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=5, classifier__n_estimators=300, classifier__subsample=0.7; total time= 2.5min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__max_depth=5, classifier__n_estimators=100, classifier__subsample=0.7; total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/17 18:19:31 INFO mlflow.sklearn.utils: Logging the 5 best runs, 5 runs will be omitted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "--- RÉSULTATS DU MODÈLE A++ (SMOTE + TUNING) ---\n",
      "Meilleur Score F1-Macro (CV) : 0.4976\n",
      "Meilleurs Hyperparamètres :\n",
      "{'classifier__subsample': 0.7, 'classifier__n_estimators': 300, 'classifier__max_depth': 5, 'classifier__learning_rate': 0.1}\n",
      "---\n",
      "Score Baseline (Modèle A, 4 feats): 0.40 F1-Macro\n",
      "Score (Modèle A+, 9 feats): 0.41 F1-Macro\n",
      "Score actuel (Modèle A++, SMOTE+Tuned): 0.4976 F1-Macro\n",
      "---\n",
      "Lance 'mlflow ui' dans ton terminal pour voir le dashboard de tous les essais !\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV # <-- On change\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# ▼▼▼ ON IMPORTE SMOTE ET LA PIPELINE IMBLEARN ▼▼▼\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# ▲▲▲ ON IMPORTE SMOTE ▲▲▲\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, f1_score, accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow # <-- On importe MLFlow\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# df_ml est ton DataFrame de la cellule précédente\n",
    "print(f\"DataFrame 'df_ml' (Cohorte 2018) chargé. Shape: {df_ml.shape}\")\n",
    "\n",
    "# --- 1. DÉFINITION DES FEATURES (On garde les 9 'Élite') ---\n",
    "DEMO_FEATURES_PLUS = [\n",
    "    \"categorieJuridiqueUniteLegale\",\n",
    "    \"trancheEffectifsUniteLegale\",\n",
    "    \"activitePrincipaleUniteLegale\",\n",
    "    \"categorieEntreprise\",\n",
    "    \"economieSocialeSolidaireUniteLegale\",\n",
    "    \"moisCreation\",\n",
    "    \"departement\",\n",
    "    \"trancheEffectifsSiege\",\n",
    "    \"caractereEmployeurSiege\"\n",
    "]\n",
    "TARGET = \"is_failed_in_3y\"\n",
    "\n",
    "X = df_ml.select(DEMO_FEATURES_PLUS).fill_null(\"INCONNU\").to_pandas()\n",
    "y = df_ml.select(TARGET).to_pandas().squeeze()\n",
    "print(f\"Features (X) sélectionnées: {len(DEMO_FEATURES_PLUS)}\")\n",
    "\n",
    "# --- 2. PRÉPARATION (Preprocessing) ---\n",
    "print(\"Preprocessing avec OneHotEncoder...\")\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False, min_frequency=0.001) # On ignore les catégories trop rares\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[(\"cat\", categorical_transformer, DEMO_FEATURES_PLUS)],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "# --- 3. CRÉATION DE LA PIPELINE \"MONSTRUEUSE\" (AVEC SMOTE) ---\n",
    "print(\"Création de la pipeline (Preprocessor + SMOTE + XGBoost)...\")\n",
    "model_A_plus = ImbPipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE(random_state=42)), # <-- L'ITÉRATION MAGIQUE\n",
    "    ('classifier', XGBClassifier( \n",
    "        # On n'a plus besoin de scale_pos_weight\n",
    "        use_label_encoder=False, \n",
    "        eval_metric='logloss',\n",
    "        random_state=42,\n",
    "        enable_categorical=False \n",
    "    ))\n",
    "])\n",
    "\n",
    "# --- 4. DÉFINIR LA GRILLE DE TUNING ---\n",
    "# On va tester 10 combinaisons au hasard\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__max_depth': [3, 5, 7, 10],\n",
    "    'classifier__learning_rate': [0.1, 0.05, 0.01],\n",
    "    'classifier__subsample': [0.7, 1.0]\n",
    "}\n",
    "\n",
    "# --- 5. ENTRAÎNEMENT & ÉVALUATION (RANDOMIZED SEARCH) ---\n",
    "print(\"Lancement du 'Monstrous' Tuning (RandomizedSearchCV)...\")\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# On optimise pour le F1-Score ! (C'est notre métrique \"business\")\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model_A_plus,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=10, # 10 essais (rapide, mais tu peux monter à 20-30)\n",
    "    cv=kfold,\n",
    "    scoring='f1_macro', # <-- ON OPTIMISE POUR LE F1\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# --- 6. CONFIGURER MLFLOW ---\n",
    "mlflow.set_experiment(\"Projet_SIRENE_Classification_Finale\")\n",
    "mlflow.sklearn.autolog() # Va logger tous les essais du RandomizedSearch\n",
    "\n",
    "print(\"Lancement de l'entraînement (peut prendre 10-20 minutes)...\")\n",
    "with mlflow.start_run() as run:\n",
    "    random_search.fit(X, y)\n",
    "    mlflow.log_param(\"model_type\", \"Model_A_plus_SMOTE_Tuned\")\n",
    "    mlflow.log_metric(\"best_f1_macro\", random_search.best_score_)\n",
    "\n",
    "# --- 7. RÉSULTATS (PLUS COMPLETS) ---\n",
    "print(\"---\")\n",
    "print(\"--- RÉSULTATS DU MODÈLE A++ (SMOTE + TUNING) ---\")\n",
    "print(f\"Meilleur Score F1-Macro (CV) : {random_search.best_score_:.4f}\")\n",
    "print(\"Meilleurs Hyperparamètres :\")\n",
    "print(random_search.best_params_)\n",
    "print(\"---\")\n",
    "print(f\"Score Baseline (Modèle A, 4 feats): 0.40 F1-Macro\")\n",
    "print(f\"Score (Modèle A+, 9 feats): 0.41 F1-Macro\")\n",
    "print(f\"Score actuel (Modèle A++, SMOTE+Tuned): {random_search.best_score_:.4f} F1-Macro\")\n",
    "print(\"---\")\n",
    "print(\"Lance 'mlflow ui' dans ton terminal pour voir le dashboard de tous les essais !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/17 18:28:03 INFO mlflow.tracking.fluent: Experiment with name 'Projet_SIRENE_Classification_LGBM' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement et préparation de 'df_ml'...\n",
      "Fichier 'sirene_infos_CLEAN.parquet' chargé. Shape: (465989, 15)\n",
      "Features (X) sélectionnées: 9\n",
      "Preprocessing avec OrdinalEncoder...\n",
      "Création de la pipeline (OrdinalEncoder + SMOTE + LightGBM)...\n",
      "Lancement du 'LGBM' Tuning (RandomizedSearchCV)...\n",
      "Lancement de l'entraînement...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Number of positive: 366726, number of negative: 366726\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021447 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 633\n",
      "[LightGBM] [Info] Number of data points in the train set: 733452, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Number of positive: 366662, number of negative: 366662\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031418 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 645\n",
      "[LightGBM] [Info] Number of data points in the train set: 733324, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Number of positive: 366691, number of negative: 366691\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Number of positive: 366732, number of negative: 366732\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036392 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 645\n",
      "[LightGBM] [Info] Number of data points in the train set: 733382, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029376 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 634\n",
      "[LightGBM] [Info] Number of data points in the train set: 733464, number of used features: 9\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 366693, number of negative: 366693\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037821 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 636\n",
      "[LightGBM] [Info] Number of data points in the train set: 733386, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Number of positive: 366726, number of negative: 366726\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045974 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 633\n",
      "[LightGBM] [Info] Number of data points in the train set: 733452, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Number of positive: 366662, number of negative: 366662\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058423 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 645\n",
      "[LightGBM] [Info] Number of data points in the train set: 733324, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Number of positive: 366691, number of negative: 366691\n",
      "[LightGBM] [Info] Number of positive: 366732, number of negative: 366732\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058858 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 645\n",
      "[LightGBM] [Info] Number of data points in the train set: 733382, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058079 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 634\n",
      "[LightGBM] [Info] Number of data points in the train set: 733464, number of used features: 9\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 366693, number of negative: 366693\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035209 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 636\n",
      "[LightGBM] [Info] Number of data points in the train set: 733386, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 366726, number of negative: 366726\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047195 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 633\n",
      "[LightGBM] [Info] Number of data points in the train set: 733452, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Number of positive: 366662, number of negative: 366662\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008293 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 645\n",
      "[LightGBM] [Info] Number of data points in the train set: 733324, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__learning_rate=0.01, classifier__max_depth=-1, classifier__n_estimators=100, classifier__num_leaves=50, classifier__subsample=1.0; total time=  11.9s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__max_depth=-1, classifier__n_estimators=100, classifier__num_leaves=50, classifier__subsample=1.0; total time=  11.9s\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Number of positive: 366691, number of negative: 366691\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012332 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 645\n",
      "[LightGBM] [Info] Number of data points in the train set: 733382, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__learning_rate=0.01, classifier__max_depth=-1, classifier__n_estimators=100, classifier__num_leaves=50, classifier__subsample=1.0; total time=  12.2s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__max_depth=-1, classifier__n_estimators=100, classifier__num_leaves=50, classifier__subsample=1.0; total time=  12.3s\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Number of positive: 366693, number of negative: 366693\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012905 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 636\n",
      "[LightGBM] [Info] Number of data points in the train set: 733386, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Number of positive: 366732, number of negative: 366732\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048032 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 634\n",
      "[LightGBM] [Info] Number of data points in the train set: 733464, number of used features: 9\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 366726, number of negative: 366726\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046626 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 633\n",
      "[LightGBM] [Info] Number of data points in the train set: 733452, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__learning_rate=0.01, classifier__max_depth=-1, classifier__n_estimators=100, classifier__num_leaves=50, classifier__subsample=1.0; total time=  15.4s\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Number of positive: 366662, number of negative: 366662\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010547 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 645\n",
      "[LightGBM] [Info] Number of data points in the train set: 733324, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=-1, classifier__n_estimators=300, classifier__num_leaves=31, classifier__subsample=0.7; total time=  21.2s\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Number of positive: 366691, number of negative: 366691\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012606 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 645\n",
      "[LightGBM] [Info] Number of data points in the train set: 733382, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=-1, classifier__n_estimators=300, classifier__num_leaves=31, classifier__subsample=0.7; total time=  28.4s\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Number of positive: 366732, number of negative: 366732\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011201 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 634\n",
      "[LightGBM] [Info] Number of data points in the train set: 733464, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OrdinalEncoder \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from lightgbm import LGBMClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# --- 1. CHARGER ET PRÉPARER LES DONNÉES (CORRIGÉ) ---\n",
    "print(\"Chargement et préparation de 'df_ml'...\")\n",
    "# ▼▼▼ ON UTILISE LE BON FICHIER \"CLEAN\" ▼▼▼\n",
    "PATH_MASTER_DEMO = \"../Data/processed/sirene_infos_CLEAN.parquet\"\n",
    "# ▲▲▲ ON UTILISE LE BON FICHIER \"CLEAN\" ▲▲▲\n",
    "\n",
    "try:\n",
    "    df_master = pl.read_parquet(PATH_MASTER_DEMO)\n",
    "    print(f\"Fichier 'sirene_infos_CLEAN.parquet' chargé. Shape: {df_master.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERREUR FATALE: Fichier non trouvé : {PATH_MASTER_DEMO}\")\n",
    "    print(\"Relance ton notebook de Data Prep pour le créer.\")\n",
    "    raise\n",
    "\n",
    "# On filtre la Cohorte 2018\n",
    "df_ml = df_master.filter(\n",
    "    pl.col(\"dateCreationUniteLegale\").dt.year() == 2018\n",
    ").with_columns(\n",
    "    # On crée la Target\n",
    "    (pl.col(\"dateCreationUniteLegale\").dt.offset_by(\"3y\")).alias(\"date_limite_3_ans\")\n",
    ").with_columns(\n",
    "    pl.when(\n",
    "        (pl.col(\"dateFermeture\").is_not_null()) & \n",
    "        (pl.col(\"dateFermeture\") < pl.col(\"date_limite_3_ans\"))\n",
    "    ).then(1)\n",
    "    .otherwise(0)\n",
    "    .alias(\"is_failed_in_3y\")\n",
    ")\n",
    "\n",
    "# --- 2. DÉFINITION DES FEATURES (Les 9 features \"Démo\") ---\n",
    "# (On prend toutes les features qu'on a créées dans le script 01)\n",
    "DEMO_FEATURES_PLUS = [\n",
    "    \"categorieJuridiqueUniteLegale\",\n",
    "    \"trancheEffectifsUniteLegale\",\n",
    "    \"activitePrincipaleUniteLegale\",\n",
    "    \"categorieEntreprise\",\n",
    "    \"economieSocialeSolidaireUniteLegale\",\n",
    "    # \"societeMissionUniteLegale\", # On l'a oubliée dans le script 01\n",
    "    \"departement\",\n",
    "    pl.col(\"dateCreationUniteLegale\").dt.month().alias(\"moisCreation\"),\n",
    "    \"trancheEffectifsSiege\",\n",
    "    \"caractereEmployeurSiege\"\n",
    "]\n",
    "TARGET = \"is_failed_in_3y\"\n",
    "\n",
    "# On doit extraire les noms (strings) pour le preprocessor\n",
    "DEMO_FEATURES_NAMES = [\n",
    "    f.meta.output_name() if isinstance(f, pl.Expr) else f for f in DEMO_FEATURES_PLUS\n",
    "]\n",
    "\n",
    "X = df_ml.select(DEMO_FEATURES_PLUS).fill_null(\"INCONNU\").to_pandas()\n",
    "y = df_ml.select(TARGET).to_pandas().squeeze()\n",
    "print(f\"Features (X) sélectionnées: {len(DEMO_FEATURES_NAMES)}\")\n",
    "\n",
    "# --- 3. PRÉPARATION (OrdinalEncoder) ---\n",
    "print(\"Preprocessing avec OrdinalEncoder...\")\n",
    "categorical_features = DEMO_FEATURES_NAMES\n",
    "categorical_transformer = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1) \n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[(\"cat\", categorical_transformer, categorical_features)],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "# --- 4. CRÉATION DE LA PIPELINE \"LGBM\" (AVEC SMOTE) ---\n",
    "print(\"Création de la pipeline (OrdinalEncoder + SMOTE + LightGBM)...\")\n",
    "categorical_feature_indices = [X.columns.get_loc(c) for c in categorical_features]\n",
    "\n",
    "model_LGBM = ImbPipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE(random_state=42)), \n",
    "    ('classifier', LGBMClassifier( \n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        categorical_feature=categorical_feature_indices\n",
    "    ))\n",
    "])\n",
    "\n",
    "# --- 5. DÉFINIR LA GRILLE DE TUNING (pour LGBM) ---\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__max_depth': [5, 7, 10, -1],\n",
    "    'classifier__learning_rate': [0.1, 0.05, 0.01],\n",
    "    'classifier__subsample': [0.7, 1.0],\n",
    "    'classifier__num_leaves': [31, 50, 70]\n",
    "}\n",
    "\n",
    "# --- 6. ENTRAÎNEMENT & ÉVALUATION (RANDOMIZED SEARCH) ---\n",
    "print(\"Lancement du 'LGBM' Tuning (RandomizedSearchCV)...\")\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model_LGBM,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=10,\n",
    "    cv=kfold,\n",
    "    scoring='f1_macro',\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# --- 7. CONFIGURER MLFLOW ---\n",
    "mlflow.set_experiment(\"Projet_SIRENE_Classification_LGBM\")\n",
    "mlflow.sklearn.autolog() \n",
    "\n",
    "print(\"Lancement de l'entraînement...\")\n",
    "with mlflow.start_run() as run:\n",
    "    random_search.fit(X, y)\n",
    "    mlflow.log_param(\"model_type\", \"Model_LGBM_SMOTE_Tuned\")\n",
    "    mlflow.log_metric(\"best_f1_macro\", random_search.best_score_)\n",
    "\n",
    "# --- 8. RÉSULTATS (PLUS COMPLETS) ---\n",
    "print(\"---\")\n",
    "print(\"--- RÉSULTATS DU MODÈLE LGBM (SMOTE + TUNING) ---\")\n",
    "print(f\"Meilleur Score F1-Macro (CV) : {random_search.best_score_:.4f}\")\n",
    "print(\"Meilleurs Hyperparamètres :\")\n",
    "print(random_search.best_params_)\n",
    "print(\"---\")\n",
    "print(f\"Score (Modèle A++, XGB+OHE): 0.4976 F1-Macro\")\n",
    "print(f\"Score actuel (Modèle LGBM+Ordinal): {random_search.best_score_:.4f} F1-Macro\")\n",
    "print(\"---\")\n",
    "print(\"Lance 'mlflow ui' dans ton terminal pour voir le dashboard de tous les essais !\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

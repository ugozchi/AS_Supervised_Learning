{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, cross_validate # <-- ON CHANGE L'IMPORT\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, f1_score, accuracy_score # <-- ON IMPORTE LES SCORES\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# df_ml est ton DataFrame de la cellule précédente\n",
    "if 'df_ml' not in locals():\n",
    "    print(\"ERREUR: 'df_ml' n'est pas chargé. Exécute la cellule 'Création de la Cohorte' d'abord.\")\n",
    "    raise NameError(\"df_ml is not defined\")\n",
    "else:\n",
    "    print(f\"DataFrame 'df_ml' (Cohorte 2018) chargé. Shape: {df_ml.shape}\")\n",
    "\n",
    "\n",
    "# --- 1. DÉFINITION DES FEATURES (ITÉRATION J : \"DÉMO MONSTRUEUSE\") ---\n",
    "print(\"Définition des features pour le Modèle A+ (Démo Étendue)...\")\n",
    "\n",
    "# (On garde la liste des 9 features qui marchent)\n",
    "DEMO_FEATURES_PLUS = [\n",
    "    \"categorieJuridiqueUniteLegale\",\n",
    "    \"trancheEffectifsUniteLegale\",\n",
    "    \"activitePrincipaleUniteLegale\",\n",
    "    \"categorieEntreprise\",\n",
    "    \"economieSocialeSolidaireUniteLegale\",\n",
    "    # \"societeMissionUniteLegale\", # On l'a oubliée dans le script 01, on la laisse de côté\n",
    "    \"moisCreation\",\n",
    "    \"departement\",\n",
    "    \"trancheEffectifsSiege\",\n",
    "    \"caractereEmployeurSiege\"\n",
    "]\n",
    "\n",
    "TARGET = \"is_failed_in_3y\"\n",
    "\n",
    "X = df_ml.select(DEMO_FEATURES_PLUS).fill_null(\"INCONNU\").to_pandas()\n",
    "y = df_ml.select(TARGET).to_pandas().squeeze()\n",
    "\n",
    "print(f\"Features (X) sélectionnées: {X.columns.to_list()}\")\n",
    "print(f\"Target (y) sélectionnée: {y.name}\")\n",
    "\n",
    "# --- 2. GESTION DU DÉSÉQUILIBRE ---\n",
    "scale_pos_weight = y.value_counts()[0] / y.value_counts()[1]\n",
    "print(f\"Ratio de déséquilibre : {scale_pos_weight:.2f}\")\n",
    "\n",
    "# --- 3. PRÉPARATION (Preprocessing) ---\n",
    "print(\"Preprocessing avec OneHotEncoder...\")\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[(\"cat\", categorical_transformer, DEMO_FEATURES_PLUS)],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "# --- 4. CRÉATION DE LA PIPELINE DE MODÉLISATION ---\n",
    "print(\"Création de la pipeline (Preprocessor + XGBoost)...\")\n",
    "model_A_plus = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', XGBClassifier(\n",
    "        scale_pos_weight=scale_pos_weight, \n",
    "        use_label_encoder=False, \n",
    "        eval_metric='logloss',\n",
    "        random_state=42,\n",
    "        enable_categorical=False \n",
    "    ))\n",
    "])\n",
    "\n",
    "# --- 5. ENTRAÎNEMENT & ÉVALUATION (CROSS-VALIDATION \"MONSTRUEUSE\") ---\n",
    "# ▼▼▼ ON CHANGE TOUTE CETTE PARTIE ▼▼▼\n",
    "print(\"Lancement de la K-Fold Cross-Validation (k=5) (AUC, Accuracy, F1)...\")\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# On définit les 3 scores qu'on veut\n",
    "scoring_metrics = {\n",
    "    'roc_auc': 'roc_auc',\n",
    "    'accuracy': 'accuracy',\n",
    "    'f1_macro': make_scorer(f1_score, average='macro') # On prend f1_macro pour le déséquilibre\n",
    "}\n",
    "\n",
    "# On utilise 'cross_validate' (pas 'cross_val_score')\n",
    "scores = cross_validate(\n",
    "    model_A_plus, \n",
    "    X, y, \n",
    "    cv=kfold, \n",
    "    scoring=scoring_metrics, \n",
    "    n_jobs=-1\n",
    ")\n",
    "# ▲▲▲ ON CHANGE TOUTE CETTE PARTIE ▲▲▲\n",
    "\n",
    "# --- 6. RÉSULTATS (PLUS COMPLETS) ---\n",
    "print(\"---\")\n",
    "print(\"--- RÉSULTATS DU MODÈLE A+ (ITÉRATION 'DÉMO MONSTRUEUSE') ---\")\n",
    "print(f\"Score ROC-AUC MOYEN (CV) : {np.mean(scores['test_roc_auc']):.4f} (+/- {np.std(scores['test_roc_auc']):.4f})\")\n",
    "print(f\"Score Accuracy MOYEN (CV) : {np.mean(scores['test_accuracy']):.4f} (+/- {np.std(scores['test_accuracy']):.4f})\")\n",
    "print(f\"Score F1-Macro MOYEN (CV) : {np.mean(scores['test_f1_macro']):.4f} (+/- {np.std(scores['test_f1_macro']):.4f})\")\n",
    "print(\"---\")\n",
    "print(f\"Score Baseline (Modèle A, 4 features): 0.7619 AUC, 0.40 F1-Macro\")\n",
    "print(f\"Score actuel (Modèle A+, {len(DEMO_FEATURES_PLUS)} features): {np.mean(scores['test_roc_auc']):.4f} AUC, {np.mean(scores['test_f1_macro']):.4f} F1-Macro\")\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV # <-- On change\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# ▼▼▼ ON IMPORTE SMOTE ET LA PIPELINE IMBLEARN ▼▼▼\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# ▲▲▲ ON IMPORTE SMOTE ▲▲▲\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, f1_score, accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow # <-- On importe MLFlow\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# df_ml est ton DataFrame de la cellule précédente\n",
    "print(f\"DataFrame 'df_ml' (Cohorte 2018) chargé. Shape: {df_ml.shape}\")\n",
    "\n",
    "# --- 1. DÉFINITION DES FEATURES (On garde les 9 'Élite') ---\n",
    "DEMO_FEATURES_PLUS = [\n",
    "    \"categorieJuridiqueUniteLegale\",\n",
    "    \"trancheEffectifsUniteLegale\",\n",
    "    \"activitePrincipaleUniteLegale\",\n",
    "    \"categorieEntreprise\",\n",
    "    \"economieSocialeSolidaireUniteLegale\",\n",
    "    \"moisCreation\",\n",
    "    \"departement\",\n",
    "    \"trancheEffectifsSiege\",\n",
    "    \"caractereEmployeurSiege\"\n",
    "]\n",
    "TARGET = \"is_failed_in_3y\"\n",
    "\n",
    "X = df_ml.select(DEMO_FEATURES_PLUS).fill_null(\"INCONNU\").to_pandas()\n",
    "y = df_ml.select(TARGET).to_pandas().squeeze()\n",
    "print(f\"Features (X) sélectionnées: {len(DEMO_FEATURES_PLUS)}\")\n",
    "\n",
    "# --- 2. PRÉPARATION (Preprocessing) ---\n",
    "print(\"Preprocessing avec OneHotEncoder...\")\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False, min_frequency=0.001) # On ignore les catégories trop rares\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[(\"cat\", categorical_transformer, DEMO_FEATURES_PLUS)],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "# --- 3. CRÉATION DE LA PIPELINE \"MONSTRUEUSE\" (AVEC SMOTE) ---\n",
    "print(\"Création de la pipeline (Preprocessor + SMOTE + XGBoost)...\")\n",
    "model_A_plus = ImbPipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE(random_state=42)), # <-- L'ITÉRATION MAGIQUE\n",
    "    ('classifier', XGBClassifier( \n",
    "        # On n'a plus besoin de scale_pos_weight\n",
    "        use_label_encoder=False, \n",
    "        eval_metric='logloss',\n",
    "        random_state=42,\n",
    "        enable_categorical=False \n",
    "    ))\n",
    "])\n",
    "\n",
    "# --- 4. DÉFINIR LA GRILLE DE TUNING ---\n",
    "# On va tester 10 combinaisons au hasard\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__max_depth': [3, 5, 7, 10],\n",
    "    'classifier__learning_rate': [0.1, 0.05, 0.01],\n",
    "    'classifier__subsample': [0.7, 1.0]\n",
    "}\n",
    "\n",
    "# --- 5. ENTRAÎNEMENT & ÉVALUATION (RANDOMIZED SEARCH) ---\n",
    "print(\"Lancement du 'Monstrous' Tuning (RandomizedSearchCV)...\")\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# On optimise pour le F1-Score ! (C'est notre métrique \"business\")\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model_A_plus,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=10, # 10 essais (rapide, mais tu peux monter à 20-30)\n",
    "    cv=kfold,\n",
    "    scoring='f1_macro', # <-- ON OPTIMISE POUR LE F1\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# --- 6. CONFIGURER MLFLOW ---\n",
    "mlflow.set_experiment(\"Projet_SIRENE_Classification_Finale\")\n",
    "mlflow.sklearn.autolog() # Va logger tous les essais du RandomizedSearch\n",
    "\n",
    "print(\"Lancement de l'entraînement (peut prendre 10-20 minutes)...\")\n",
    "with mlflow.start_run() as run:\n",
    "    random_search.fit(X, y)\n",
    "    mlflow.log_param(\"model_type\", \"Model_A_plus_SMOTE_Tuned\")\n",
    "    mlflow.log_metric(\"best_f1_macro\", random_search.best_score_)\n",
    "\n",
    "# --- 7. RÉSULTATS (PLUS COMPLETS) ---\n",
    "print(\"---\")\n",
    "print(\"--- RÉSULTATS DU MODÈLE A++ (SMOTE + TUNING) ---\")\n",
    "print(f\"Meilleur Score F1-Macro (CV) : {random_search.best_score_:.4f}\")\n",
    "print(\"Meilleurs Hyperparamètres :\")\n",
    "print(random_search.best_params_)\n",
    "print(\"---\")\n",
    "print(f\"Score Baseline (Modèle A, 4 feats): 0.40 F1-Macro\")\n",
    "print(f\"Score (Modèle A+, 9 feats): 0.41 F1-Macro\")\n",
    "print(f\"Score actuel (Modèle A++, SMOTE+Tuned): {random_search.best_score_:.4f} F1-Macro\")\n",
    "print(\"---\")\n",
    "print(\"Lance 'mlflow ui' dans ton terminal pour voir le dashboard de tous les essais !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OrdinalEncoder \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from lightgbm import LGBMClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# --- 1. CHARGER ET PRÉPARER LES DONNÉES (CORRIGÉ) ---\n",
    "print(\"Chargement et préparation de 'df_ml'...\")\n",
    "# ▼▼▼ ON UTILISE LE BON FICHIER \"CLEAN\" ▼▼▼\n",
    "PATH_MASTER_DEMO = \"../Data/processed/sirene_infos_CLEAN.parquet\"\n",
    "# ▲▲▲ ON UTILISE LE BON FICHIER \"CLEAN\" ▲▲▲\n",
    "\n",
    "try:\n",
    "    df_master = pl.read_parquet(PATH_MASTER_DEMO)\n",
    "    print(f\"Fichier 'sirene_infos_CLEAN.parquet' chargé. Shape: {df_master.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERREUR FATALE: Fichier non trouvé : {PATH_MASTER_DEMO}\")\n",
    "    print(\"Relance ton notebook de Data Prep pour le créer.\")\n",
    "    raise\n",
    "\n",
    "# On filtre la Cohorte 2018\n",
    "df_ml = df_master.filter(\n",
    "    pl.col(\"dateCreationUniteLegale\").dt.year() == 2018\n",
    ").with_columns(\n",
    "    # On crée la Target\n",
    "    (pl.col(\"dateCreationUniteLegale\").dt.offset_by(\"3y\")).alias(\"date_limite_3_ans\")\n",
    ").with_columns(\n",
    "    pl.when(\n",
    "        (pl.col(\"dateFermeture\").is_not_null()) & \n",
    "        (pl.col(\"dateFermeture\") < pl.col(\"date_limite_3_ans\"))\n",
    "    ).then(1)\n",
    "    .otherwise(0)\n",
    "    .alias(\"is_failed_in_3y\")\n",
    ")\n",
    "\n",
    "# --- 2. DÉFINITION DES FEATURES (Les 9 features \"Démo\") ---\n",
    "# (On prend toutes les features qu'on a créées dans le script 01)\n",
    "DEMO_FEATURES_PLUS = [\n",
    "    \"categorieJuridiqueUniteLegale\",\n",
    "    \"trancheEffectifsUniteLegale\",\n",
    "    \"activitePrincipaleUniteLegale\",\n",
    "    \"categorieEntreprise\",\n",
    "    \"economieSocialeSolidaireUniteLegale\",\n",
    "    # \"societeMissionUniteLegale\", # On l'a oubliée dans le script 01\n",
    "    \"departement\",\n",
    "    pl.col(\"dateCreationUniteLegale\").dt.month().alias(\"moisCreation\"),\n",
    "    \"trancheEffectifsSiege\",\n",
    "    \"caractereEmployeurSiege\"\n",
    "]\n",
    "TARGET = \"is_failed_in_3y\"\n",
    "\n",
    "# On doit extraire les noms (strings) pour le preprocessor\n",
    "DEMO_FEATURES_NAMES = [\n",
    "    f.meta.output_name() if isinstance(f, pl.Expr) else f for f in DEMO_FEATURES_PLUS\n",
    "]\n",
    "\n",
    "X = df_ml.select(DEMO_FEATURES_PLUS).fill_null(\"INCONNU\").to_pandas()\n",
    "y = df_ml.select(TARGET).to_pandas().squeeze()\n",
    "print(f\"Features (X) sélectionnées: {len(DEMO_FEATURES_NAMES)}\")\n",
    "\n",
    "# --- 3. PRÉPARATION (OrdinalEncoder) ---\n",
    "print(\"Preprocessing avec OrdinalEncoder...\")\n",
    "categorical_features = DEMO_FEATURES_NAMES\n",
    "categorical_transformer = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1) \n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[(\"cat\", categorical_transformer, categorical_features)],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "# --- 4. CRÉATION DE LA PIPELINE \"LGBM\" (AVEC SMOTE) ---\n",
    "print(\"Création de la pipeline (OrdinalEncoder + SMOTE + LightGBM)...\")\n",
    "categorical_feature_indices = [X.columns.get_loc(c) for c in categorical_features]\n",
    "\n",
    "model_LGBM = ImbPipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE(random_state=42)), \n",
    "    ('classifier', LGBMClassifier( \n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        categorical_feature=categorical_feature_indices\n",
    "    ))\n",
    "])\n",
    "\n",
    "# --- 5. DÉFINIR LA GRILLE DE TUNING (pour LGBM) ---\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__max_depth': [5, 7, 10, -1],\n",
    "    'classifier__learning_rate': [0.1, 0.05, 0.01],\n",
    "    'classifier__subsample': [0.7, 1.0],\n",
    "    'classifier__num_leaves': [31, 50, 70]\n",
    "}\n",
    "\n",
    "# --- 6. ENTRAÎNEMENT & ÉVALUATION (RANDOMIZED SEARCH) ---\n",
    "print(\"Lancement du 'LGBM' Tuning (RandomizedSearchCV)...\")\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model_LGBM,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=10,\n",
    "    cv=kfold,\n",
    "    scoring='f1_macro',\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# --- 7. CONFIGURER MLFLOW ---\n",
    "mlflow.set_experiment(\"Projet_SIRENE_Classification_LGBM\")\n",
    "mlflow.sklearn.autolog() \n",
    "\n",
    "print(\"Lancement de l'entraînement...\")\n",
    "with mlflow.start_run() as run:\n",
    "    random_search.fit(X, y)\n",
    "    mlflow.log_param(\"model_type\", \"Model_LGBM_SMOTE_Tuned\")\n",
    "    mlflow.log_metric(\"best_f1_macro\", random_search.best_score_)\n",
    "\n",
    "# --- 8. RÉSULTATS (PLUS COMPLETS) ---\n",
    "print(\"---\")\n",
    "print(\"--- RÉSULTATS DU MODÈLE LGBM (SMOTE + TUNING) ---\")\n",
    "print(f\"Meilleur Score F1-Macro (CV) : {random_search.best_score_:.4f}\")\n",
    "print(\"Meilleurs Hyperparamètres :\")\n",
    "print(random_search.best_params_)\n",
    "print(\"---\")\n",
    "print(f\"Score (Modèle A++, XGB+OHE): 0.4976 F1-Macro\")\n",
    "print(f\"Score actuel (Modèle LGBM+Ordinal): {random_search.best_score_:.4f} F1-Macro\")\n",
    "print(\"---\")\n",
    "print(\"Lance 'mlflow ui' dans ton terminal pour voir le dashboard de tous les essais !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV # <-- On garde RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# --- 1. CHARGER LE DATASET ML FINAL (LE COMPLET) ---\n",
    "print(\"Chargement du dataset ML 'CLEAN'...\")\n",
    "PATH_ML_CLEAN = \"../Data/processed/sirene_infos_CLEAN.parquet\"\n",
    "\n",
    "try:\n",
    "    df_master = pl.read_parquet(PATH_ML_CLEAN)\n",
    "    print(f\"Dataset chargé. Shape: {df_master.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERREUR: Fichier '{PATH_ML_CLEAN}' non trouvé.\")\n",
    "    raise e\n",
    "\n",
    "# --- 2. FILTRER LA COHORTE 2018 ET CRÉER LA TARGET ---\n",
    "print(\"Filtrage Cohorte 2018 et création de la Target...\")\n",
    "df_ml = df_master.filter(\n",
    "    pl.col(\"dateCreationUniteLegale\").dt.year() == 2018\n",
    ").with_columns(\n",
    "    (pl.col(\"dateCreationUniteLegale\").dt.offset_by(\"3y\")).alias(\"date_limite_3_ans\")\n",
    ").with_columns(\n",
    "    pl.when(\n",
    "        (pl.col(\"dateFermeture\").is_not_null()) & \n",
    "        (pl.col(\"dateFermeture\") < pl.col(\"date_limite_3_ans\"))\n",
    "    ).then(1)\n",
    "    .otherwise(0)\n",
    "    .alias(\"is_failed_in_3y\")\n",
    ")\n",
    "\n",
    "# --- 3. DÉFINITION DES FEATURES (Les 9 \"Élite\") ---\n",
    "DEMO_FEATURES_PLUS = [\n",
    "    \"categorieJuridiqueUniteLegale\",\n",
    "    \"trancheEffectifsUniteLegale\",\n",
    "    \"activitePrincipaleUniteLegale\",\n",
    "    \"categorieEntreprise\",\n",
    "    \"economieSocialeSolidaireUniteLegale\",\n",
    "    \"moisCreation\",\n",
    "    \"departement\",\n",
    "    \"trancheEffectifsSiege\",\n",
    "    \"caractereEmployeurSiege\"\n",
    "]\n",
    "TARGET = \"is_failed_in_3y\"\n",
    "\n",
    "X = df_ml.select(DEMO_FEATURES_PLUS).fill_null(\"INCONNU\").to_pandas()\n",
    "y = df_ml.select(TARGET).to_pandas().squeeze()\n",
    "print(f\"Features (X) sélectionnées: {len(DEMO_FEATURES_PLUS)}\")\n",
    "\n",
    "# --- 4. PRÉPARATION (Preprocessing - OHE) ---\n",
    "# On a prouvé que OHE + XGBoost marchait mieux\n",
    "print(\"Preprocessing avec OneHotEncoder...\")\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False, min_frequency=0.01) # min_frequency aide à réduire le bruit\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[(\"cat\", categorical_transformer, DEMO_FEATURES_PLUS)],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "# --- 5. CRÉATION DE LA PIPELINE (OHE + SMOTE + XGBoost) ---\n",
    "print(\"Création de la pipeline (OHE + SMOTE + XGB Classifier)...\")\n",
    "\n",
    "# On utilise ImbPipeline pour que SMOTE ne s'applique qu'au train set dans la CV\n",
    "model_pipeline = ImbPipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE(random_state=42)), \n",
    "    ('classifier', XGBClassifier( \n",
    "        use_label_encoder=False, \n",
    "        eval_metric='logloss',\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# --- 6. DÉFINIR LA GRILLE DE TUNING (pour XGBoost) ---\n",
    "# On va tester des paramètres différents\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [100, 200, 400], # Nb d'arbres\n",
    "    'classifier__max_depth': [3, 5, 8],          # Profondeur\n",
    "    'classifier__learning_rate': [0.05, 0.1, 0.2],\n",
    "    'classifier__subsample': [0.7, 0.8, 1.0],\n",
    "    'classifier__colsample_bytree': [0.7, 0.8, 1.0] # % de features utilisées par arbre\n",
    "}\n",
    "\n",
    "# --- 7. ENTRAÎNEMENT & ÉVALUATION (RANDOMIZED SEARCH) ---\n",
    "print(\"Lancement du 'XGBoost Tuning' (RandomizedSearchCV)...\")\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=42) # On met k=3 pour aller plus vite\n",
    "\n",
    "# On teste 10 combinaisons au hasard\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model_pipeline,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=10, \n",
    "    cv=kfold,\n",
    "    scoring='f1_macro', # On optimise pour le F1\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# --- 8. CONFIGURER MLFLOW ---\n",
    "mlflow.set_experiment(\"Projet_SIRENE_Classification_XGB_Tuned\")\n",
    "mlflow.sklearn.autolog() \n",
    "\n",
    "print(\"Lancement de l'entraînement (peut prendre 10-20 minutes)...\")\n",
    "with mlflow.start_run() as run:\n",
    "    random_search.fit(X, y)\n",
    "    mlflow.log_param(\"model_type\", \"Model_A_plus_plus_XGB_SMOTE_Tuned\")\n",
    "    mlflow.log_metric(\"best_f1_macro\", random_search.best_score_)\n",
    "\n",
    "# --- 9. RÉSULTATS (PLUS COMPLETS) ---\n",
    "print(\"---\")\n",
    "print(\"--- RÉSULTATS DU MODÈLE A++ (XGBoost + SMOTE + TUNING) ---\")\n",
    "print(f\"Meilleur Score F1-Macro (CV) : {random_search.best_score_:.4f}\")\n",
    "print(\"Meilleurs Hyperparamètres :\")\n",
    "print(random_search.best_params_)\n",
    "print(\"---\")\n",
    "print(f\"Score précédent (Modèle A++, non-tuné): 0.4976 F1-Macro\")\n",
    "print(f\"Score actuel (Modèle A++, Tuné): {random_search.best_score_:.4f} F1-Macro\")\n",
    "print(\"---\")\n",
    "print(\"Lance 'mlflow ui' dans ton terminal pour voir le dashboard de tous les essais !\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

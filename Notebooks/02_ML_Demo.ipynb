{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, recall_score, precision_score, f1_score\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Début du Fichier main.py ---\n",
    "\n",
    "def load_data(file_path='../Data/processed/sirene_infos_CLEAN.parquet'):\n",
    "    \"\"\"Charge les données et sépare X et y.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_parquet(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erreur: Le fichier {file_path} est introuvable.\")\n",
    "        return None, None, None\n",
    "\n",
    "    # 1. Définition de la variable cible (y)\n",
    "    y = df['is_failed_in_3y']\n",
    "\n",
    "    # 2. Définition des features (X) et exclusion des colonnes de fuite\n",
    "    leakage_columns = ['is_failed_in_3y', 'dateFermeture', 'date_limite_3_ans', 'siren']\n",
    "    X = df.drop(columns=leakage_columns, errors='ignore')\n",
    "\n",
    "    # Conversion de la date au bon format si ce n'est pas déjà fait\n",
    "    if 'dateCreationUniteLegale' in X.columns:\n",
    "        X['dateCreationUniteLegale'] = pd.to_datetime(X['dateCreationUniteLegale'])\n",
    "\n",
    "    return X, y, df\n",
    "\n",
    "# X, y, df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_split(X, y, split_date='2018-01-01'):\n",
    "    \"\"\"Effectue un split d'entraînement et de test basé sur le temps.\"\"\"\n",
    "    \n",
    "    split_dt = pd.to_datetime(split_date)\n",
    "    \n",
    "    # Séparation : Train = avant la date, Test = à partir de la date\n",
    "    train_mask = X['dateCreationUniteLegale'] < split_dt\n",
    "    \n",
    "    X_train = X[train_mask].copy()\n",
    "    y_train = y[train_mask].copy()\n",
    "    \n",
    "    X_test = X[~train_mask].copy()\n",
    "    y_test = y[~train_mask].copy()\n",
    "    \n",
    "    print(f\"\\n--- Split Temporel Effectué (Coupure: {split_date}) ---\")\n",
    "    print(f\"Taille du Train: {len(X_train)} entreprises.\")\n",
    "    print(f\"Taille du Test: {len(X_test)} entreprises.\")\n",
    "    print(f\"Proportion de défaillances (1) dans Train: {y_train.mean():.2%}\")\n",
    "    print(f\"Proportion de défaillances (1) dans Test: {y_test.mean():.2%}\")\n",
    "    \n",
    "    # Retirer la date de création de X_train et X_test après le split\n",
    "    # (Elle est maintenant redondante avec anneeCreation/moisCreation)\n",
    "    X_train = X_train.drop(columns=['dateCreationUniteLegale'])\n",
    "    X_test = X_test.drop(columns=['dateCreationUniteLegale'])\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_preprocessor(X_train):\n",
    "    \"\"\"Définit le pré-processing pour les différents types de colonnes.\"\"\"\n",
    "    \n",
    "    # Variables Numériques (anneeCreation, moisCreation)\n",
    "    numeric_features = ['anneeCreation', 'moisCreation']\n",
    "    \n",
    "    # Variables Catégorielles (toutes les autres après retrait de numeric/date)\n",
    "    # On enlève toutes les colonnes numériques/temporelles restantes\n",
    "    categorical_features = [col for col in X_train.columns if col not in numeric_features]\n",
    "\n",
    "    # Pipeline pour les variables numériques\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler()) # Standardisation des variables\n",
    "    ])\n",
    "    \n",
    "    # Pipeline pour les variables catégorielles (One-Hot Encoding)\n",
    "    # handle_unknown='ignore' permet de gérer les nouvelles catégories dans le test set\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "    \n",
    "    # Combinaison des transformations\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ],\n",
    "        remainder='drop' # Supprime toute colonne non mentionnée\n",
    "    )\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "# preprocessor = build_preprocessor(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_baseline_pipeline(preprocessor):\n",
    "    \"\"\"Construit et retourne le pipeline complet de baseline (Log. Reg.).\"\"\"\n",
    "    \n",
    "    # Le modèle baseline : Régression Logistique avec gestion du déséquilibre\n",
    "    # class_weight='balanced' ajuste automatiquement les poids\n",
    "    # proportionnellement aux fréquences des classes dans les données d'entraînement.\n",
    "    baseline_model = LogisticRegression(\n",
    "        random_state=42, \n",
    "        solver='liblinear', \n",
    "        class_weight='balanced'\n",
    "    )\n",
    "    \n",
    "    # Pipeline complet : Pré-processing -> Modèle\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', baseline_model)\n",
    "    ])\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "# pipeline = build_baseline_pipeline(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(pipeline, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Entraîne le modèle et évalue les métriques sur le test set.\"\"\"\n",
    "    \n",
    "    print(\"\\n--- Entraînement du Modèle Baseline ---\")\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"--- Évaluation des Métriques sur le Test Set ---\")\n",
    "    \n",
    "    # Prédictions et Probabilités\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    # Nous utilisons predict_proba pour la métrique ROC AUC\n",
    "    y_proba = pipeline.predict_proba(X_test)[:, 1] # Probabilité de la classe 1 (faillite)\n",
    "    \n",
    "    # Calcul des métriques\n",
    "    metrics = {\n",
    "        \"ROC AUC\": roc_auc_score(y_test, y_proba),\n",
    "        \"Recall (Rappel)\": recall_score(y_test, y_pred), # Important pour ne pas rater les défaillances\n",
    "        \"Precision (Précision)\": precision_score(y_test, y_pred, zero_division=0),\n",
    "        \"F1-Score\": f1_score(y_test, y_pred),\n",
    "    }\n",
    "\n",
    "    print(\"\\n✅ Résultats du Baseline sur l'Ensemble de Test:\")\n",
    "    for name, value in metrics.items():\n",
    "        print(f\"   * {name}: {value:.4f}\")\n",
    "        \n",
    "    return metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
